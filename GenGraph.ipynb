{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "oSAqtCu_QkFj",
        "BrR0MY_OQcDZ",
        "bOZWtoDg0y0v",
        "3usg8PjL8oqh"
      ],
      "mount_file_id": "1ig_My8CYbUkklkJIw1af4vN2QgZHGCv0",
      "authorship_tag": "ABX9TyNQ4vUNwB3NMMnWCWiE0mHR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menezeslarissa/gen-graph/blob/main/GenGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOi-q9OKfQVs"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-datasets"
      ],
      "metadata": {
        "id": "F6qFK6Qa4YcP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZOjpZLzs0xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = 'AIzaSyAi-CR9gEE8fwkfCs0TF256qzCa5lj2gNU'"
      ],
      "metadata": {
        "id": "H5_iRuOYzGEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bibliotecas necessarias\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import os\n",
        "%matplotlib inline\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import zipfile\n",
        "import tarfile\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "from tenacity import retry, stop_after_attempt\n",
        "import os\n",
        "import time\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g1RWw1t22Mw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "id": "5B9TRtSLGd93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErEmeIjPQXUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Refinement"
      ],
      "metadata": {
        "id": "oSAqtCu_QkFj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fTPhkyFm5FZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Step: Gemini for Text Refinement\n",
        "\n",
        "Prompt: Please generate a refined document of the\n",
        "following document. And please ensure that\n",
        "the refined document meets the following\n",
        "criteria:\n",
        "1. The refined document should be abstract and\n",
        "does not change any original meaning of\n",
        "the document.\n",
        "2. The refined document should retain all the\n",
        "important objects, concepts, and\n",
        "relationships between them.\n",
        "3. The refined document should only contain\n",
        "information that is from the document.\n",
        "4. The refined document should be readable and\n",
        "easy to understand without any\n",
        "abbreviations and misspellings."
      ],
      "metadata": {
        "id": "bMJV-Nr3y8D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "refine_text_prompt_base = '''Please generate a refined document of the\n",
        "following document. And please ensure that\n",
        "the refined document meets the following\n",
        "criteria:\n",
        "1. Ensure the refined version remains abstract and does not change the original meaning.\n",
        "2. Retain all important elements, concepts, and relationships from the original document.\n",
        "3. Do not add, remove, or modify information beyond what exists in the original content.\n",
        "4. The refined version must be clear, concise, and easy to read, with no abbreviations or spelling errors.\n",
        "5. Present the content in a structured, organized format for better readability.\n",
        "Here is the content to refine: '''"
      ],
      "metadata": {
        "id": "gfvZUTaF6fPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response_ref.text)"
      ],
      "metadata": {
        "id": "uws7WQI16nk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second Step: Gemini for Knowledge Extraction (Identify entities, relationships and interactions)\n",
        "\n",
        "Prompt: You are a knowledge graph extractor, and your\n",
        "task is to extract and return a knowledge\n",
        "graph from a given text.Let’s extract it\n",
        "step by step:\n",
        "(1). Identify the entities in the text. An\n",
        "entity can be a noun or a noun phrase that\n",
        "refers to a real-world object or an\n",
        "abstract concept. You can use a named\n",
        "entity recognition (NER) tool or a part-of\n",
        "-speech (POS) tagger to identify the\n",
        "entities.\n",
        "(2). Identify the relationships between the\n",
        "entities. A relationship can be a verb or\n",
        "a prepositional phrase that connects two\n",
        "entities. You can use dependency parsing\n",
        "to identify the relationships.\n",
        "(3). Summarize each entity and relation as\n",
        "short as possible and remove any stop\n",
        "words.\n",
        "(4). Only return the knowledge graph in the\n",
        "triplet format: (’head entity’, ’relation\n",
        "’, ’tail entity’).\n",
        "(5). Most importantly, if you cannot find any\n",
        "knowledge, please just output: 'None'"
      ],
      "metadata": {
        "id": "Lbn5V86uzuYS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgK11fB5zqL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knowlegde_extractor_prompt_base = '''You are a knowledge graph extractor, and your\n",
        "task is to extract and return a knowledge\n",
        "graph from a given text.Let’s extract it\n",
        "step by step:\n",
        "(1). Identify the entities in the text. An\n",
        "entity can be a noun or a noun phrase that\n",
        "refers to a real-world object or an\n",
        "abstract concept. You can use a named\n",
        "entity recognition (NER) tool or a part-of\n",
        "-speech (POS) tagger to identify the\n",
        "entities. No need to return the answer for this step.\n",
        "(2). Identify the relationships between the\n",
        "entities. A relationship can be a verb or\n",
        "a prepositional phrase that connects two\n",
        "entities. You can use dependency parsing\n",
        "to identify the relationships. No need to return the answer for this step.\n",
        "(3). Summarize each entity and relation as\n",
        "short as possible and remove any stop\n",
        "words. No need to return the answer for this step.\n",
        "(4). The response of the knowledge graph must be the\n",
        "triplet format given below:\n",
        "(’head entity’, ’relation\n",
        "’, ’tail entity’).\n",
        "(5). Most importantly, if you cannot find any\n",
        "knowledge, please just output: 'None'.\n",
        "Here is the content: '''"
      ],
      "metadata": {
        "id": "vyB9PzKa7Bty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QCK0vf-xnul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knowlegde_extractor_prompt_base = '''You are a knowledge graph extractor. Your task is to extract and return a knowledge graph from the provided text. Follow these steps carefully:\n",
        "\n",
        "1. Entity Identification: Identify entities in the text. An entity is a noun or noun phrase that refers to a real-world object or an abstract concept. You may use tools like named entity recognition (NER) or part-of-speech (POS) tagging. Do not return the results for this step.\n",
        "\n",
        "2. Relationship Identification: Identify relationships between the entities. A relationship is typically represented by a verb or prepositional phrase connecting two entities. Dependency parsing can help identify these relationships. Do not return the results for this step.\n",
        "\n",
        "3. Refinement: Summarize entities and relationships as succinctly as possible, removing any unnecessary stop words. Do not return the results for this step.\n",
        "\n",
        "4. Output Format: Present the knowledge graph in the following triplet format:\n",
        "\n",
        "('head entity', 'relation', 'tail entity')\n",
        "Handling Empty Results: If no valid knowledge graph can be extracted, output only: None.\n",
        "\n",
        "Input Text: '''"
      ],
      "metadata": {
        "id": "rMFZGufVxowj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(teste_gemini_ref)"
      ],
      "metadata": {
        "id": "Ep5umCNS7H1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)"
      ],
      "metadata": {
        "id": "X98NlazW7Lk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Third Step: Convert Knowledge to Graph Structure\n"
      ],
      "metadata": {
        "id": "S0BaXbpr0th-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to visualize data\n",
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "                     node_color=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KNZCCdDj3b9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuração api key Gemini"
      ],
      "metadata": {
        "id": "BrR0MY_OQcDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "import os\n"
      ],
      "metadata": {
        "id": "RRUxpkZDzZUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "GOOGLE_API_KEY='AIzaSyBypCj2Jd9YhcFlZYXiuyt2mH4ua_AtfSg'\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XuoBodK1z8Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temperature = controls the randomness of the output. Use higher values for more creative responses, and lower values for more deterministic responses.\n",
        "\n",
        "config = genai.types.GenerationConfig(\n",
        "        # Only one candidate for now.\n",
        "        candidate_count=1,\n",
        "        # max_output_tokens=300,\n",
        "        temperature=0.1,\n",
        "    )"
      ],
      "metadata": {
        "id": "Oa16ZUXWz_7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "# version = 'gemini-1.5-pro-exp-0827' # versoa experiemntal de testes\n",
        "version = 'gemini-1.5-flash' # versoa experiemntal de testes\n",
        "# version = 'gemini-1.5-pro' # versoa experiemntal de testes\n",
        "model = genai.GenerativeModel(version)"
      ],
      "metadata": {
        "id": "FLsrQeQhy6t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%time\n",
        "response = model.generate_content(\"What is the meaning of life?\", generation_config=config)"
      ],
      "metadata": {
        "id": "Z1vU_9lP1fxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "aI5RJK2dGPQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(stop=stop_after_attempt(3)) # <-- here's the key\n",
        "def get_response(text:str) -> str:\n",
        "  response = model.generate_content(text)\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "uombl8zY82f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_response('what is gnn?'))\n"
      ],
      "metadata": {
        "id": "J2KzX0ek1nKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSITe5RABhvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuração Vertex AI"
      ],
      "metadata": {
        "id": "bOZWtoDg0y0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "UI4mM1wW1ElD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "from google.cloud import aiplatform\n",
        "\n"
      ],
      "metadata": {
        "id": "HvJQCXzh0xjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aiplatform.init(\n",
        "    # your Google Cloud Project ID or number\n",
        "    # environment default used is not set\n",
        "    project='mestrado-unesp',\n",
        "\n",
        "    # the Vertex AI region you will use\n",
        "    # defaults to us-central1\n",
        "    location='us-central1',\n",
        "\n",
        "    # Google Cloud Storage bucket in same region as location\n",
        "    # used to stage artifacts\n",
        "    staging_bucket='gs://my_staging_bucket',\n",
        "\n",
        "    # custom google.auth.credentials.Credentials\n",
        "    # # environment default credentials used if not set\n",
        "    # credentials=my_credentials,\n",
        "\n",
        "    # # customer managed encryption key resource name\n",
        "    # # will be applied to all Vertex AI resources if set\n",
        "    # encryption_spec_key_name=my_encryption_key_name,\n",
        "\n",
        "    # the name of the experiment to use to track\n",
        "    # logged metrics and parameters\n",
        "    experiment='my-experiment',\n",
        "\n",
        "    # description of the experiment above\n",
        "    experiment_description='my experiment description'\n",
        ")"
      ],
      "metadata": {
        "id": "arJC068q3k7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt):\n",
        "  vertexai.init(location=\"us-central1\")\n",
        "  model = GenerativeModel(\n",
        "    \"gemini-1.5-pro-001\",\n",
        "  )\n",
        "  responses = model.generate_content(\n",
        "      [prompt],\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=True,\n",
        "  )\n",
        "\n",
        "  return \"\".join([response.text for response in responses])\n",
        "\n",
        "generation_config = {\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "}"
      ],
      "metadata": {
        "id": "L_XjskaO1Om8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html ."
      ],
      "metadata": {
        "id": "7-wiXlUqKdDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentos"
      ],
      "metadata": {
        "id": "Bw3X1pU2QXly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features: \t3 (Sci/Tech)\n",
        "\t1 (Sports)\n",
        "\t2 (Business) and 0 (World)\n"
      ],
      "metadata": {
        "id": "6qDygX_sO4vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds, info = tfds.load('ag_news_subset', split='train', shuffle_files=True, as_supervised=True, with_info=True)\n",
        "assert isinstance(ds, tf.data.Dataset)\n"
      ],
      "metadata": {
        "id": "WgOCV9T9JlpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tfds.as_dataframe(ds, info)"
      ],
      "metadata": {
        "id": "c_l3InhJoGCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "id": "qYDAD38zJ-rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.sample(2)"
      ],
      "metadata": {
        "id": "wM9yDUnJsYkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save raw data\n",
        "pd.DataFrame(ds).to_csv(os.path.join('/content/drive/MyDrive/mestrado/data/ag_news_subset',r'raw_data.csv'))\n"
      ],
      "metadata": {
        "id": "fUynFkYin8Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get raw data training\n",
        "raw_data_train_df = pd.read_csv(\"/content/drive/MyDrive/mestrado/data/ag_news_subset/raw_data.csv\")"
      ],
      "metadata": {
        "id": "030v_HwzsAUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_train_df.sample(2)"
      ],
      "metadata": {
        "id": "ifQyZHQ4sqgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_docs = raw_data_train_df.values.tolist()\n"
      ],
      "metadata": {
        "id": "d6cjTu99PZXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sample_docs)"
      ],
      "metadata": {
        "id": "W8rbYZbatirh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building vocab\n",
        "word_freq = {}\n",
        "for doc_words in sample_docs:\n",
        "    words = doc_words[1].split()\n",
        "    for word in words:\n",
        "        word = word  # Decode the word outside the loop\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "vocab = list(word_freq.keys())\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "O1OPrOmOklsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "id": "TkWbLkBnttFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_doc_list = {}\n",
        "\n",
        "for i, doc_words in enumerate(sample_docs):  # Use enumerate for direct index access\n",
        "    words = doc_words[1].split()\n",
        "    for word in words:\n",
        "        word = word  # Decode the word\n",
        "        if word not in word_doc_list:\n",
        "            word_doc_list[word] = []  # Initialize list for new words\n",
        "        if i not in word_doc_list[word]:  # Check for duplicates before appending\n",
        "            word_doc_list[word].append(i)"
      ],
      "metadata": {
        "id": "1UJ0LJsVky8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_doc_list.keys()"
      ],
      "metadata": {
        "id": "c3b9zn1XmKHn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = tfidf_vec.fit_transform(word_doc_list.keys())\n",
        "tfidf_matrix_array = tfidf_matrix.toarray()\n",
        "print(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))"
      ],
      "metadata": {
        "id": "tlBalkv0mSAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = []\n",
        "\n",
        "for i in range(len(vocab)):\n",
        "    word = vocab[i]\n",
        "    vector = tfidf_matrix_array[i]\n",
        "    str_vector = []\n",
        "    for j in range(len(vector)):\n",
        "        str_vector.append(str(vector[j]))\n",
        "    temp = ' '.join(str_vector)\n",
        "    word_vector = word + ' ' + temp\n",
        "    word_vectors.append(word_vector)\n"
      ],
      "metadata": {
        "id": "gZI9-7zkoBpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing labels list\n",
        "label_set = set()\n",
        "for doc in sample_docs:\n",
        "  label_set.add(doc[1])\n",
        "\n",
        "label_list = list(label_set)"
      ],
      "metadata": {
        "id": "pgc72i1DoIFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract entities\n"
      ],
      "metadata": {
        "id": "rlnmyCSepINO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wU9BqXyUpNrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
        "    real_train_size, dim))"
      ],
      "metadata": {
        "id": "dy6FZZ8FqwJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_truple(text):\n",
        "  triplets = []\n",
        "  for line in text.split('\\n'):\n",
        "    # Remove parentheses and split by comma\n",
        "    parts = line.strip('()').split(', ')\n",
        "    # Remove quotes and append to triplets list\n",
        "    triplets.append(tuple([part.strip(\"'\") for part in parts]))\n",
        "  return triplets"
      ],
      "metadata": {
        "id": "lskf-JZ55T3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the graph knopwlegdes as text truples\n",
        "df = pd.DataFrame(columns=['head', 'relation', 'tail', 'label'])\n"
      ],
      "metadata": {
        "id": "aeCSGLZh48BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triplets = []"
      ],
      "metadata": {
        "id": "bpYgITDT5kL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhfUMpfIveuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first store the refined text in a csv and save in google *drive*"
      ],
      "metadata": {
        "id": "pbf_c3zFsuGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # TODO: guardar em csv\n",
        "refined_text_df = pd.DataFrame(columns=['original_text', 'refined_text', 'label'])\n",
        "\n",
        "for idx, doc in enumerate(sample_docs):\n",
        "  prompt_refined = refine_text_prompt_base + doc[1]\n",
        "  response_ref = model.generate_content(prompt_refined, safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    })\n",
        "\n",
        "  if(idx == 8):\n",
        "    print('sleeping')\n",
        "    time.sleep(60)\n",
        "  if(idx == 999):\n",
        "    break\n",
        "\n",
        "  if(response_ref):\n",
        "    print(response_ref)\n",
        "    text = response_ref.text\n",
        "    new_row = {'original_text': doc[1], 'refined_text': text, 'label': doc[2]}\n",
        "    refined_text_df = pd.concat([refined_text_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "refined_text_df.to_csv(os.path.join('/content/drive/MyDrive/mestrado/data/ag_news_subset',r'refined_text_df.csv'))\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QVcjsM1xuCKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refined_text_df.to_csv(os.path.join('/content/drive/MyDrive/mestrado/data/ag_news_subset',r'refined_text_df_train.csv'))\n"
      ],
      "metadata": {
        "id": "A6SILohW02H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(refined_text_df)"
      ],
      "metadata": {
        "id": "J_7bD1Vj041n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we'll process the refined text and extract the knowledge graph and then store it in a csv and save in google drive"
      ],
      "metadata": {
        "id": "mMkVV3soxTU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "refinedtext_train_df = pd.read_csv(\"/content/drive/MyDrive/mestrado/data/ag_news_subset/refined_text_df_train.csv\")\n"
      ],
      "metadata": {
        "id": "4qVlUGQci55b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: guardar em csv\n",
        "extracted_kowledge = {}\n",
        "for idx, row in refinedtext_train_df.iterrows():\n",
        "  doc = row.values # Access the row values as a NumPy array\n",
        "  refined_text = doc[2]\n",
        "  prompt_knowledge_extctr = knowlegde_extractor_prompt_base + refined_text\n",
        "  response_ke = model.generate_content(prompt_knowledge_extctr, safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    })\n",
        "\n",
        "  if(response_ke.text != 'None'):\n",
        "    text = response_ke.text\n",
        "    triplets = get_truple(text)\n",
        "\n",
        "    # Check if triplets is a list of tuples with 3 elements before unpacking\n",
        "    for triplet in triplets:\n",
        "      if isinstance(triplet, tuple) and len(triplet) == 3: # Verify triplet structure\n",
        "          h, r, t = triplet\n",
        "          new_row = {'head': h, 'relation': r, 'tail': t, 'label': doc[3]}\n",
        "          print(new_row)\n",
        "          df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "      else:\n",
        "          print(f\"Skipping invalid triplet: {triplet}\")  # Log or handle invalid triplets\n",
        "\n",
        "\n",
        "  if(idx == 8):\n",
        "    print('sleeping')\n",
        "    time.sleep(60)\n",
        "  if(idx == 999):\n",
        "    break\n",
        "\n",
        "# todo: adicionar head relation tail num csv e dps adicionar na gnn , cada linha com head relation e taikl"
      ],
      "metadata": {
        "id": "x2uPu1EZMiXv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix some typos in the data\n",
        "typos = {\n",
        "            r\"\\*\\s?\\(\\'\": \"\",       # Matches \"* ('\" or \"*('\"\n",
        "            r\"\\'\\)\": \"\",            # Matches \"')\"\n",
        "            r\"\\*\\s?\\*\\*\\(\\'\": \"\",   # Matches \"* **('\"\n",
        "            r\"\\(\\'\": \"\",            # Matches \"('\"\n",
        "            r\"\\'\": \"\",              # Matches standalone single quotes\n",
        "        }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pI3kb-TA43Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  # Columns to clean\n",
        "  columns_to_clean = ['head', 'relation', 'tail']\n",
        "\n",
        "  # Apply typo corrections across specified columns\n",
        "  for column in columns_to_clean:\n",
        "      df_copy[column] = df_copy[column].astype(str)\n",
        "      for pattern, correction in typos.items():\n",
        "          df_copy[column] = df_copy[column].str.replace(pattern, correction, regex=True)\n",
        "\n",
        "  return df_copy"
      ],
      "metadata": {
        "id": "YuRY_oeb3L2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_ke_df = clean_data(df)\n"
      ],
      "metadata": {
        "id": "nc1fHSi63VQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_ke_df.sample(20)"
      ],
      "metadata": {
        "id": "wnXLUprJ3ctX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_ke_df.to_csv(os.path.join('/content/drive/MyDrive/mestrado/data/ag_news_subset',r'final_ke_df_train.csv'))\n"
      ],
      "metadata": {
        "id": "mpt7aepT3j40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_ke_df = pd.read_csv(\"/content/drive/MyDrive/mestrado/data/ag_news_subset/final_ke_df_train.csv\")\n"
      ],
      "metadata": {
        "id": "VC-PbcmBjSc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying with reuters dataset\n",
        "final_ke_df = pd.read_csv(\"/content/drive/MyDrive/mestrado/data/reuters_dataset/kb_reuters_train_df.csv\")\n"
      ],
      "metadata": {
        "id": "rckrYiWnLaBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "import ast\n",
        "\n",
        "for _, row in final_ke_df.iterrows():\n",
        "  label_value = ast.literal_eval(row['label'])[0]\n",
        "  data_list.append({\n",
        "      'head': row['head'],\n",
        "      'relation': row['relation'],\n",
        "      'tail': row['tail'],\n",
        "      'label': label_value\n",
        "  })\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nbqJFCWRIBhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get list of labels\n"
      ],
      "metadata": {
        "id": "OhboiqTrNbvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-4rMoh40I7xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for reuters we have to convert to one hot encoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
        "enc.fit(X)\n",
        "enc.categories_\n",
        "enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
        "enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
        "enc.get_feature_names_out(['gender', 'group'])"
      ],
      "metadata": {
        "id": "fykVTWDlNLKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. Build node mapping for head and tail\n",
        "nodes = set()  # Use a set to ensure unique nodes\n",
        "for entry in data_list:\n",
        "    nodes.add(entry['head'])  # Add head node\n",
        "    nodes.add(entry['tail'])  # Add tail node\n",
        "\n",
        "# 2. Map nodes to indices\n",
        "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
        "\n",
        "# 3. Initialize edge list and node labels\n",
        "edges = []  # List to store edges\n",
        "node_labels = torch.zeros(len(nodes), dtype=torch.long)  # Initialize node labels tensor\n",
        "\n",
        "# 4. Process each entry to create edges and assign labels to nodes\n",
        "for entry in data_list:\n",
        "    # Get indices for head and tail nodes\n",
        "    head_idx = node_to_idx[entry['head']]\n",
        "    tail_idx = node_to_idx[entry['tail']]\n",
        "\n",
        "    # Add edge (head_idx, tail_idx) to edges list\n",
        "    edges.append([head_idx, tail_idx])\n",
        "\n",
        "    # Assign label to head and tail nodes\n",
        "    node_labels[head_idx] = entry['label']\n",
        "    node_labels[tail_idx] = entry['label']\n",
        "\n",
        "# 5. Convert edges to torch tensor and transpose to match GCN format\n",
        "edges = torch.tensor(edges, dtype=torch.long).t().contiguous()  # Edge list (head, tail) -> (source, target)\n",
        "\n",
        "# 6. Create feature matrix (one-hot encoding of nodes)\n",
        "num_nodes = len(node_to_idx)\n",
        "x = torch.eye(num_nodes, dtype=torch.float)  # Identity matrix for one-hot encoding\n",
        "\n",
        "# 7. Convert node labels to a tensor\n",
        "y = node_labels  # Use node labels for classification\n",
        "\n",
        "# 8. Create graph data object\n",
        "data = Data(x=x, edge_index=edges, y=y)\n",
        "\n"
      ],
      "metadata": {
        "id": "T7DG14LWGKL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "p-yWZCcsQS-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the GCN model\n",
        "model = GCN(num_node_features=data.x.shape[1], num_classes=torch.max(data.y) + 1)  # Adjust num_classes based on the labels\n",
        "\n",
        "# Initialize optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "8vhW-VMhlt8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 10. Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    out = model(data)\n",
        "\n",
        "    # Calculate loss (use only the nodes that have labels, i.e., data.y is not -1)\n",
        "    loss = criterion(out, data.y)  # Cross entropy loss\n",
        "\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Optimizer step\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "wqN0Kug-oCg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(data)\n",
        "    _, pred = out.max(dim=1)  # Get the predicted labels\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (pred == data.y).float().mean().item()\n",
        "\n",
        "    # Precision, Recall, F1-Score\n",
        "    precision = precision_score(data.y.cpu(), pred.cpu(), average='weighted')\n",
        "    recall = recall_score(data.y.cpu(), pred.cpu(), average='weighted')\n",
        "    f1 = f1_score(data.y.cpu(), pred.cpu(), average='weighted')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(data.y.cpu(), pred.cpu())\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "    print('Confusion Matrix:')\n",
        "    print(conf_matrix)"
      ],
      "metadata": {
        "id": "6QefOBDZpNGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# # True labels and predicted labels (you can replace these with your own)\n",
        "# true_labels = data.y.cpu().numpy()  # Assuming data.y is your ground truth\n",
        "# predicted_labels = pred.cpu().numpy()  # Assuming pred is your model's predictions\n",
        "\n",
        "# # Compute the confusion matrix\n",
        "# cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 1', 'Class 2', 'Class 3', 'Class 4'],\n",
        "            yticklabels=['Class 1', 'Class 2', 'Class 3', 'Class 4'], cbar=False)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rxYYiRMurq2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now the we have the train data kets create the graph\n",
        "\n",
        "ref: https://networkx.org/documentation/stable/tutorial.html"
      ],
      "metadata": {
        "id": "6C-DLzSM5OIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_graph(df):\n",
        "  G = nx.Graph()\n",
        "  for _, row in df.iterrows():\n",
        "    G.add_edge(row['head'], row['tail'], label=row['relation'])\n",
        "  return G"
      ],
      "metadata": {
        "id": "MYGFjTAW5dNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_1 = create_graph(final_ke_df)"
      ],
      "metadata": {
        "id": "b50CjhSR5yYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TW772GyTrwGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hn67rfzI5S7U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czFPfp3J8Plu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimento Reuters Dataset"
      ],
      "metadata": {
        "id": "3NCdIRSGKvYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Download dataset, setupo train data and data cleaning"
      ],
      "metadata": {
        "id": "oeOBF4Dw5QZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')"
      ],
      "metadata": {
        "id": "fDQBHTZKS_mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "\n",
        "\n",
        "documents = reuters.fileids()\n",
        "print(str(len(documents)) + \" documents\")\n",
        "print(str(len(reuters.categories())) + \" categories:\")\n",
        "print(reuters.categories())"
      ],
      "metadata": {
        "id": "RuTdauEsOytu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dAEZa87jWjo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "df = pd.DataFrame(reuters.fileids([\"acq\", \"cpu\", \"livestock\"]), columns=['fileid'])\n",
        "df['raw'] = df['fileid'].progress_map(lambda f: reuters.raw(f))\n",
        "df['label'] = df['fileid'].progress_map(lambda f: reuters.categories(f))\n",
        "df.index = df['fileid'].progress_map(lambda f: int(f.split('/')[1]))\n",
        "df.index.name = None\n",
        "df = df.drop(columns=['fileid']).sort_index()\n",
        "\n",
        "df.sample(3, random_state=12)"
      ],
      "metadata": {
        "id": "8NsiQYxGWR03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['headline', 'raw_text']] = df.progress_apply(lambda row: row['raw'].split('\\n', 1), axis='columns', result_type='expand')"
      ],
      "metadata": {
        "id": "4IVdxIURXFJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(2)"
      ],
      "metadata": {
        "id": "eLalGI4J3vyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_data(text):\n",
        "    # html escape\n",
        "    text = text.replace('&lt;','<')\n",
        "\n",
        "    # quotation marks instead of <>\n",
        "    text = re.sub(r'[<>]', '\"', text)\n",
        "\n",
        "    # drop stock symbols\n",
        "    text = re.sub(r'[ ]*\"[A-Z\\.]+\"', '', text)\n",
        "\n",
        "    # drop stock symbols\n",
        "    text = re.sub(r'[ ]*\\([A-Z\\.]+\\)', '', text)\n",
        "\n",
        "    text = re.sub(r'\\bdlr(s?)\\b', r'dollar\\1', text, flags=re.I)\n",
        "    text = re.sub(r'\\bmln(s?)\\b', r'million\\1', text, flags=re.I)\n",
        "    text = re.sub(r'\\bpct\\b', r'%', text, flags=re.I)\n",
        "\n",
        "    # normalize INC to Inc\n",
        "    text = re.sub(r'\\b(Co|Corp|Inc|Plc|Ltd)\\b', lambda m: m.expand(r'\\1').capitalize(), text, flags=re.I)\n",
        "\n",
        "    # quotation marks\n",
        "    text = re.sub(r'\"', r'', text)\n",
        "\n",
        "    # multiple whitespace by one\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # typo\n",
        "    text = re.sub(r'acquisiton', 'acquisition', text)\n",
        "    text = re.sub(r'Nippon bLife', 'Nippon Life', text)\n",
        "\n",
        "    # missing space at end of sentence\n",
        "    text = re.sub(r'COMSAT.COMSAT', 'COMSAT. COMSAT', text)\n",
        "    #text = re.sub(r'Audio/Video', 'Audio-Video', text)\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "yF5gD7rA8ndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['raw_text'].progress_map(clean_data)\n",
        "df['headline'] = df['headline'].progress_map(clean_data)"
      ],
      "metadata": {
        "id": "b2o5Dc1uKtos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(2)"
      ],
      "metadata": {
        "id": "t_Z2sox431bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['raw_text'].progress_map(lambda t: t.isupper())][['headline', 'raw_text']].head(3)\n",
        "\n",
        "df = df[df['raw_text'].progress_map(lambda t: not t.isupper())]\n",
        "\n",
        "df[['headline', 'text']].sample(3, random_state=12)"
      ],
      "metadata": {
        "id": "vj3FJSgIXaXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reuters_df = df.copy()"
      ],
      "metadata": {
        "id": "k-zOl2_A4Ibn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reuters_df.to_csv(os.path.join('/content/drive/MyDrive/mestrado/data/reuters_dataset',r'train_reuters_ds.csv'))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0puGQAcuXeXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reuters_df['refined_text'] = ''"
      ],
      "metadata": {
        "id": "C_sXp8Ny5_xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reuters_df.head(2)"
      ],
      "metadata": {
        "id": "Do6qcWcb6DFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0ci1ti2TyjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: iterate vlaues from train_reuters_df\n",
        "\n",
        "for index, row in train_reuters_df.iterrows():\n",
        "  # Access individual values using column names\n",
        "  headline = row['headline']\n",
        "  text = row['text']\n",
        "  label = row['label']\n",
        "  # ... perform operations with the values ...\n",
        "  print(f\"Index: {index}, Headline: {headline}, Text: {text}, Label: {label}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WRM8EFC36mvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step #2 - Refine text using Gemini"
      ],
      "metadata": {
        "id": "26gnXVZi5XfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for index, row in train_reuters_df.iterrows():\n",
        "  text = row['text']\n",
        "  label = row['label']\n",
        "  prompt_refined = refine_text_prompt_base + text\n",
        "  response_ref = model.generate_content(prompt_refined, safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    })\n",
        "\n",
        "  if(index == 8):\n",
        "    print('sleeping')\n",
        "    time.sleep(60)\n",
        "  # if(idx == 999):\n",
        "  #   break\n",
        "\n",
        "  if(response_ref):\n",
        "    print(response_ref)\n",
        "    response_text = response_ref.text\n",
        "    # row['refined_text'] = response_text\n",
        "    train_reuters_df.loc[index, 'refined_text'] = response_text # add this\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OISd3qsMXpyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row"
      ],
      "metadata": {
        "id": "CmBiNNmo7mcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQL966TD7maW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reuters_df.to_csv(os.path.join('/content/drive/MyDrive/mestrado/data/reuters_dataset',r'refined_train_reuters_df.csv'))\n"
      ],
      "metadata": {
        "id": "H810Qbq07Kx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reuters_df = pd.read_csv(\"/content/drive/MyDrive/mestrado/data/reuters_dataset/refined_train_reuters_df.csv\")"
      ],
      "metadata": {
        "id": "sJZU1loYTlqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step #3 - Extract Knowledge Graph"
      ],
      "metadata": {
        "id": "NsKOH3CWIB6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_count = 0  # Initialize the iterative index\n",
        "extracted_kowledge = {}\n",
        "for idx, row in train_reuters_df.iterrows():\n",
        "  index_count += 1\n",
        "  print(f\"Processing document {index_count}\")\n",
        "  doc = row.values # Access the row values as a NumPy array\n",
        "  refined_text = doc[4]\n",
        "  prompt_knowledge_extctr = knowlegde_extractor_prompt_base + refined_text\n",
        "  response_ke = model.generate_content(prompt_knowledge_extctr, safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    })\n",
        "\n",
        "  if(response_ke.text != 'None' and index_count == 2):\n",
        "    text = response_ke.text\n",
        "    triplets = get_truple(text)\n",
        "\n",
        "  #   # Check if triplets is a list of tuples with 3 elements before unpacking\n",
        "  #   for triplet in triplets:\n",
        "  #     if isinstance(triplet, tuple) and len(triplet) == 3: # Verify triplet structure\n",
        "  #         h, r, t = triplet\n",
        "  #         new_row = {'head': h, 'relation': r, 'tail': t, 'label': doc[3]}\n",
        "  #         print(new_row)\n",
        "  #         df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "  #     else:\n",
        "  #         print(f\"Skipping invalid triplet: {triplet}\")  # Log or handle invalid triplets\n",
        "\n",
        "\n",
        "  # if(idx == 8):\n",
        "  #   print('sleeping')\n",
        "  #   time.sleep(60)\n",
        "  # if(idx == 999):\n",
        "  #   break\n"
      ],
      "metadata": {
        "id": "xYsEeZi6IAiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step #3 ALTERNATIVE - Extract Knowledge Graph with rebel\n",
        "ref: https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text"
      ],
      "metadata": {
        "id": "Hfqe9nB-LlUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pyvis"
      ],
      "metadata": {
        "id": "e_s6d90rL34Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# needed to load the REBEL model\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import math\n",
        "import torch\n",
        "\n",
        "# graph visualization\n",
        "from pyvis.network import Network\n",
        "\n",
        "# show HTML in notebook\n",
        "import IPython"
      ],
      "metadata": {
        "id": "vKHdLXevLtG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
        "model_rebel = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
      ],
      "metadata": {
        "id": "6UMPdx0rMOdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://huggingface.co/Babelscape/rebel-large\n",
        "def extract_relations_from_model_output(text):\n",
        "    relations = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
        "    for token in text_replaced.split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'relation': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'relation': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        relations.append({\n",
        "            'head': subject.strip(),\n",
        "            'relation': relation.strip(),\n",
        "            'tail': object_.strip()\n",
        "        })\n",
        "    return relations"
      ],
      "metadata": {
        "id": "sX3n9k6wMbib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# knowledge base class\n",
        "class KB():\n",
        "    def __init__(self):\n",
        "        self.relations = []\n",
        "\n",
        "    def are_relations_equal(self, r1, r2):\n",
        "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"relation\", \"tail\"])\n",
        "\n",
        "    def exists_relation(self, r1):\n",
        "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
        "\n",
        "    def add_relation(self, r):\n",
        "        if not self.exists_relation(r):\n",
        "            self.relations.append(r)\n",
        "\n",
        "    def print(self):\n",
        "        print(\"Relations:\")\n",
        "        for r in self.relations:\n",
        "            print(f\"  {r}\")"
      ],
      "metadata": {
        "id": "-eqswk2wNjDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a knowledge base from text\n",
        "def from_small_text_to_kb(text, verbose=False):\n",
        "    kb = KB()\n",
        "\n",
        "    # Tokenizer text\n",
        "    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True,\n",
        "                            return_tensors='pt')\n",
        "    if verbose:\n",
        "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
        "\n",
        "    # Generate\n",
        "    gen_kwargs = {\n",
        "        \"max_length\": 216,\n",
        "        \"length_penalty\": 0,\n",
        "        \"num_beams\": 3,\n",
        "        \"num_return_sequences\": 3\n",
        "    }\n",
        "    generated_tokens = model_rebel.generate(\n",
        "        **model_inputs,\n",
        "        **gen_kwargs,\n",
        "    )\n",
        "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
        "\n",
        "    # create kb\n",
        "    for sentence_pred in decoded_preds:\n",
        "        relations = extract_relations_from_model_output(sentence_pred)\n",
        "        for r in relations:\n",
        "            kb.add_relation(r)\n",
        "\n",
        "    return kb"
      ],
      "metadata": {
        "id": "xb8eGGCTMhzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"john was born in Brazil\"\n",
        "\n",
        "kb = from_small_text_to_kb(text, verbose=True)\n",
        "kb.print()"
      ],
      "metadata": {
        "id": "cxgSSFYPNbLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGzzM7j2WMUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data = []\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4SEkgXXTWfMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_count = 0  # Initialize the iterative index\n",
        "extracted_kowledge = {}\n",
        "for idx, row in train_reuters_df.iterrows():\n",
        "  index_count += 1\n",
        "  print(f\"Processing document {index_count}\")\n",
        "  doc = row.values # Access the row values as a NumPy array\n",
        "  refined_text = doc[4]\n",
        "  triplets = from_small_text_to_kb(refined_text, verbose=True)\n",
        "  for triplet in triplets.relations:\n",
        "    h, r, t = triplet['head'], triplet['relation'], triplet['tail']\n",
        "    new_row = {'head': h, 'relation': r, 'tail': t, 'label': doc[2], 'docid': doc[0]}\n",
        "    extracted_data.append(new_row)\n",
        "\n",
        "\n",
        "kb_reuters_train_df = pd.DataFrame(extracted_data)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mfu6JGfESQwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_reuters_train_df.head()\n"
      ],
      "metadata": {
        "id": "yoJKrCGqXHfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_reuters_train_df.to_csv(os.path.join('/content/drive/MyDrive/mestrado/data/reuters_dataset',r'kb_reuters_train_df.csv'))\n"
      ],
      "metadata": {
        "id": "shU5hkQNXNM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_reuters_train_df = pd.read_csv(\"/content/drive/MyDrive/mestrado/data/reuters_dataset/kb_reuters_train_df.csv\")"
      ],
      "metadata": {
        "id": "kYlm6dVSPQx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_reuters_data_list = []\n",
        "for _, row in kb_reuters_train_df.iterrows():\n",
        "  kb_reuters_data_list.append({\n",
        "      'head': row['head'],\n",
        "      'relation': row['relation'],\n",
        "      'tail': row['tail'],\n",
        "      'label': row['label']\n",
        "  })\n"
      ],
      "metadata": {
        "id": "jicNKP40nSxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_reuters_data_list"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aIDNdPmBnbAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "node_map = {}\n",
        "edge_index = []\n",
        "labels = {}\n",
        "label_to_id = {}  # Mapping from label strings to unique numeric IDs\n",
        "\n",
        "# Step 2: Create node mapping and build edge index\n",
        "for sample in kb_reuters_data_list:\n",
        "    head = sample['head']\n",
        "    tail = sample['tail']\n",
        "    label_str = sample['label']\n",
        "\n",
        "    label_list = ast.literal_eval(label_str)\n",
        "\n",
        "    # Ensure each label in label_list has a unique numeric ID\n",
        "    for label in label_list:\n",
        "        if label not in label_to_id:\n",
        "            label_to_id[label] = len(label_to_id)\n",
        "\n",
        "    # Map head and tail to unique IDs\n",
        "    if head not in node_map:\n",
        "        node_map[head] = len(node_map)\n",
        "        labels[node_map[head]] = [label_to_id[label] for label in label_list]  # Store all labels for the node\n",
        "\n",
        "    if tail not in node_map:\n",
        "        node_map[tail] = len(node_map)\n",
        "        labels[node_map[tail]] = [label_to_id[label] for label in label_list]  # Store all labels for the node\n",
        "\n",
        "    # Add edge (head -> tail) in the graph\n",
        "    edge_index.append((node_map[head], node_map[tail]))\n",
        "\n",
        "# Convert to a set to remove duplicates and then to tensor\n",
        "edge_index = list(set(edge_index))\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# # Update the number of edges after removing duplicates\n",
        "print(f\"Number of edges after removing duplicates: {edge_index.size(1)}\")\n",
        "\n",
        "# Node feature matrix (Identity matrix for simplicity, size is number of nodes)\n",
        "num_nodes = len(node_map)\n",
        "x = torch.eye(num_nodes, dtype=torch.float)\n",
        "\n",
        "# Create target label tensor (for each node, multi-label format)\n",
        "num_labels = len(label_to_id)\n",
        "y = torch.zeros((num_nodes, num_labels), dtype=torch.float)\n",
        "\n",
        "for node_id, label_ids in labels.items():\n",
        "    for label_id in label_ids:\n",
        "        y[node_id, label_id] = 1  # Set to 1 for each label that applies to the node\n",
        "\n",
        "# Step 3: Create PyTorch Geometric Data object\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Check the shapes and content\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "print(f\"Node features shape: {x.shape}\")\n",
        "print(f\"Node labels shape: {y.shape}\")\n",
        "print(\"Label to ID mapping:\", label_to_id)"
      ],
      "metadata": {
        "id": "whKukv4dnFNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "n1M_nuawoZVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb_reuters_data_list"
      ],
      "metadata": {
        "id": "Z7zIsgAucVlG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "all_labels = []\n",
        "for row in kb_reuters_data_list:\n",
        "    labels = ast.literal_eval(row['label'])\n",
        "    all_labels.extend(labels)\n",
        "\n",
        "print(all_labels)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "69FPZSc8pDpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = Counter(all_labels)\n",
        "print(\"Label Counts:\", label_counts)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_t8iHGunZVV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert counts to a dictionary of weights\n",
        "class_weights = {label: 1 / (count + 1) for label, count in label_counts.items()}\n",
        "print(\"Class Weights:\", class_weights)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LTxBFrtybCpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "label_map = {label: idx for idx, label in enumerate(label_counts.keys())}\n",
        "weights_array = [class_weights[label] for label in label_map.keys()]\n",
        "\n",
        "class_weights_tensor = torch.tensor(weights_array, dtype=torch.float32)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n"
      ],
      "metadata": {
        "id": "nmgQShQQc2hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ste3 Create **GCN**"
      ],
      "metadata": {
        "id": "qUQFhnM0pE4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # First GCN layer to reduce high-dimensional features to a manageable size\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
        "\n",
        "        # Second GCN layer for further processing\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Third GCN layer with the output matching the number of labels (num_classes)\n",
        "        self.conv3 = GCNConv(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GCN layer with ReLU activation and dropout\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training, p=0.2)\n",
        "\n",
        "        # Second GCN layer with ReLU activation and dropout\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training, p=0.2)\n",
        "\n",
        "        # Third GCN layer with no activation (for logits)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Sigmoid activation for multi-label classification\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Initialize model\n",
        "num_node_features = 5531  # As per your data\n",
        "hidden_dim = 128  # You can adjust based on your needs\n",
        "num_classes = 43  # Output dimension as per number of labels\n",
        "\n",
        "model = GCN(num_node_features=num_node_features, hidden_dim=hidden_dim, num_classes=num_classes)\n",
        "\n"
      ],
      "metadata": {
        "id": "RHWu2dAXp0EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gACemmxKuxbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(model, data, optimizer, criterion, scheduler=None):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear gradients\n",
        "    out = model(data)  # Forward pass\n",
        "\n",
        "    # Compute the loss using BCELoss for multi-label classification with sigmoid output\n",
        "    loss = criterion(out, data.y.float())  # Ensure labels are in float format for BCELoss\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "     # Step the scheduler if provided\n",
        "    if scheduler is not None:\n",
        "        scheduler.step(loss)  # Use the current loss to adjust the learning rate if on plateau\n",
        "\n",
        "\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "k3oyWP9Dp4BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate(model, data, threshold=0.3):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)  # Model already outputs probabilities if sigmoid is applied in the model's forward pass\n",
        "\n",
        "        # Convert probabilities to binary predictions\n",
        "        preds_binary = (out > threshold).float()\n",
        "\n",
        "        # Calculate metrics for each label\n",
        "        y_true = data.y.cpu().numpy()\n",
        "        y_pred = preds_binary.cpu().numpy()\n",
        "\n",
        "        # Calculate and print overall metrics\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "            \"precision\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "            \"recall\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "            \"f1\": f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        }\n",
        "\n",
        "        print(\"Overall Metrics:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "\n",
        "        # Metrics per label\n",
        "        label_metrics = {\n",
        "            \"precision\": precision_score(y_true, y_pred, average=None, zero_division=0),\n",
        "            \"recall\": recall_score(y_true, y_pred, average=None, zero_division=0),\n",
        "            \"f1\": f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "        }\n",
        "\n",
        "        print(\"\\nMetrics per Label:\")\n",
        "        for i in range(y_true.shape[1]):\n",
        "            print(f\"Label {i}: Precision: {label_metrics['precision'][i]:.4f}, \"\n",
        "                  f\"Recall: {label_metrics['recall'][i]:.4f}, F1: {label_metrics['f1'][i]:.4f}\")\n",
        "\n",
        "        return metrics, label_metrics\n"
      ],
      "metadata": {
        "id": "nt5g-M4hp-aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the evaluation and print results\n",
        "metrics, label_metrics = evaluate(model, data)"
      ],
      "metadata": {
        "id": "O3RZkrjZudK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.y.shape)  # Should be [5531, 43]\n"
      ],
      "metadata": {
        "id": "5znB71Zlt09A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, criterion, and optimizer\n",
        "num_node_features = data.x.shape[1]\n",
        "num_classes = data.y.shape[1]  # Number of unique labels in multi-label setup\n",
        "model = GCN(num_node_features=num_node_features, hidden_dim=hidden_dim, num_classes=num_classes)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()  # Suitable for multi-label classification\n",
        "# criterion = torch.nn.BCELoss()\n",
        "# weights = data.y.sum(dim=0) / data.y.size(0)  # Inverse frequency of labels\n",
        "# criterion = torch.nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train(model, data, optimizer, criterion, scheduler)\n",
        "\n",
        "    # Optionally print the learning rate and loss for each epoch\n",
        "    for param_group in optimizer.param_groups:\n",
        "        current_lr = param_group['lr']\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j6c0biuIo0jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming you have a dataset with a number of nodes\n",
        "num_nodes = data.num_nodes\n",
        "\n",
        "# Create a validation mask: let's say you want to randomly select 20% of the nodes for validation\n",
        "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "num_val = int(num_nodes * 0.2)  # 20% for validation\n",
        "val_mask[torch.randperm(num_nodes)[:num_val]] = 1  # Randomly select 20% of nodes for validation\n",
        "\n",
        "# Assign the mask to your data object\n",
        "data.val_mask = val_mask\n"
      ],
      "metadata": {
        "id": "gjP07J1TVcld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Ensure your predictions are in binary format (0 or 1)\n",
        "y_pred_binary = (out > 0.5).float()  # Apply threshold to convert probabilities to 0 or 1\n",
        "\n",
        "# Flatten both the true labels and predicted labels\n",
        "y_true = data.y[data.val_mask].cpu().numpy()\n",
        "y_pred = y_pred_binary[data.val_mask].cpu().numpy()\n",
        "\n",
        "# Check that both are of the same shape\n",
        "print(y_true.shape, y_pred.shape)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Validation Precision: {precision:.4f}\")\n",
        "print(f\"Validation Recall: {recall:.4f}\")\n",
        "print(f\"Validation F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "Ip4tzhZtVShg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def print_sample_predictions(model, data, threshold=0.2, num_samples=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Model prediction (raw output)\n",
        "        out = model(data)\n",
        "\n",
        "        # Apply sigmoid to convert raw output to probabilities\n",
        "        preds = torch.sigmoid(out)\n",
        "\n",
        "        # Convert probabilities to binary predictions based on the threshold\n",
        "        preds_binary = (preds > threshold).float()\n",
        "\n",
        "        # Convert true labels and predictions to numpy arrays\n",
        "        y_true = data.y.cpu().numpy()\n",
        "        y_pred = preds_binary.cpu().numpy()\n",
        "\n",
        "        # Print sample comparisons (True vs Predicted)\n",
        "        print(f\"Showing {num_samples} sample comparisons:\")\n",
        "        for i in range(num_samples):\n",
        "            print(f\"\\nSample {i+1}:\")\n",
        "            print(f\"True labels: {y_true[i]}\")\n",
        "            print(f\"Predicted labels: {y_pred[i]}\")\n",
        "            print('-' * 50)\n",
        "\n",
        "# Example usage (make sure `data` is your validation data):\n",
        "print_sample_predictions(model, data)\n"
      ],
      "metadata": {
        "id": "ZKG4Y8yvWKIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(data)  # Raw logits\n",
        "print(out[:10])  # Print the first 10 raw logits for inspection\n"
      ],
      "metadata": {
        "id": "Rx1TCBzvWvkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.sum(y_true, axis=0))  # Total occurrences of each label (column-wise sum)\n"
      ],
      "metadata": {
        "id": "TVgnxmY9QorF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of evaluation\n",
        "preds = evaluate(model, data)\n",
        "print(\"Predictions (probabilities):\", preds)"
      ],
      "metadata": {
        "id": "kvWiBZBDqno6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds."
      ],
      "metadata": {
        "id": "Lgrq0cpyqKDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEZEkW3WpJN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Embeddings for the Graph\n",
        "\n",
        "Graph embeddings are mathematical representations of nodes or edges in a graph in a continuous vector space. These embeddings capture the structural and relational information of the graph, allowing us to perform various analyses, such as node similarity calculation and visualization in lower-dimensional space."
      ],
      "metadata": {
        "id": "3usg8PjL8oqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using random wal approach. We select a random neighbor of the starting point and move to this neighbor. And we continue the process by selecting random neighbors of the current point.\n",
        "\n",
        "Why?\n",
        "\n",
        "- this method offers a flexible stochastic definition of node similarity, integrating both local and higher-order neighborhood details.\n",
        "- by focusing solely on pairs of nodes that co-occur during random walks, we eliminate the need to consider all possible node pairs during training.\n",
        "\n",
        "This process is a unsupervised feature learning procedure"
      ],
      "metadata": {
        "id": "W2hADbx3gDzm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vX4Vlfknf5v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install node2vec"
      ],
      "metadata": {
        "id": "D7z6iXlg88R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from node2vec import Node2Vec\n"
      ],
      "metadata": {
        "id": "5VAddZDqDkXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d - dimenmsionality\n",
        "# walk_len = the length of the random walks generated to sample the neighborhood of each node.\n",
        "# num_walks = number of random walks generated for each node in the graph\n",
        "# workers = number of threads used for parallel execution of random walk generation\n",
        "# window_size  = maximum distance between the target node and the context nodes within a walk that are considered for learning the embeddings.\n",
        "# batch_words  = controls the batch size for training the Word2Vec mode\n",
        "\n",
        "d, walk_len, num_walks, workers = 256, 30, 200, 4\n",
        "window_size, batch_words = 10, 4"
      ],
      "metadata": {
        "id": "DCkZpSptCbmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "from node2vec import Node2Vec\n",
        "\n",
        "def get_node_embeddings(graph):\n",
        "  \"\"\"\n",
        "  Generates node embeddings using node2vec.\n",
        "\n",
        "  This function now handles NaN values in node IDs by checking\n",
        "  if the node ID is a float and then if it's NaN using np.isnan.\n",
        "  If the node ID is not a float, it assumes it's not NaN.\n",
        "\n",
        "  Args:\n",
        "      graph: The input graph.\n",
        "\n",
        "  Returns:\n",
        "      The trained node2vec model.\n",
        "  \"\"\"\n",
        "  # Remove nodes with NaN IDs\n",
        "  graph.remove_nodes_from([node for node in graph.nodes() if isinstance(node, float) and np.isnan(node)])\n",
        "\n",
        "  # Convert node IDs to strings if necessary\n",
        "  # This is crucial if node IDs are not strings (e.g., numbers with NaN values)\n",
        "  graph = nx.relabel_nodes(graph, {node: str(node) for node in graph.nodes()})\n",
        "\n",
        "  # Generate node embeddings using node2vec\n",
        "  node2vec = Node2Vec(graph, dimensions=d, walk_length=walk_len, num_walks=num_walks, workers=workers)\n",
        "  model = node2vec.fit(window=window_size, min_count=1, batch_words=batch_words) # Training the model\n",
        "  return model"
      ],
      "metadata": {
        "id": "tcMEqwekBRUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_train = get_node_embeddings(graph_1)"
      ],
      "metadata": {
        "id": "yhb6rrurDcrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = {node: model_train.wv[node] for node in graph_1.nodes()}\n"
      ],
      "metadata": {
        "id": "FlwXR65EHD4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4OEPlPNtHGtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df = pd.DataFrame(list(embeddings.items()), columns=['node', 'embedding'])\n"
      ],
      "metadata": {
        "id": "sz8QkQOjEM4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df"
      ],
      "metadata": {
        "id": "N6vddOJ2Hd_v",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df.to_csv(os.path.join('/content/drive/MyDrive/mestrado/data/ag_news_subset',r'final_ke_df_nd_embeddings_train.csv'))\n"
      ],
      "metadata": {
        "id": "pnD81yuKD5_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mrWog4BkDOit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next step: # Define the adjacency matrix (edge connections)\n"
      ],
      "metadata": {
        "id": "8hsKtzJfKD45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_train = pd.read_csv(\"/content/drive/MyDrive/mestrado/data/ag_news_subset/final_ke_df_nd_embeddings_train.csv\")"
      ],
      "metadata": {
        "id": "9SvBDlj1DpVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desired embedding length (set to 256 as per your requirement)\n",
        "embedding_length = 256\n",
        "\n",
        "# Function to extract embeddings and pad or truncate them to a fixed length of 256\n",
        "def process_embedding(embedding_str, expected_length):\n",
        "    # Check if the embedding is in bytes and decode it\n",
        "    if isinstance(embedding_str, bytes):\n",
        "        embedding_str = embedding_str.decode(\"utf-8\")  # Decode byte string to regular string\n",
        "\n",
        "    # Extract float values from the string representation of embeddings\n",
        "    values = [float(val) for val in re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", embedding_str)]\n",
        "\n",
        "    # If the embedding is shorter, pad with zeros\n",
        "    if len(values) < expected_length:\n",
        "        values = values + [0.0] * (expected_length - len(values))\n",
        "\n",
        "    # If the embedding is longer, truncate it\n",
        "    elif len(values) > expected_length:\n",
        "        values = values[:expected_length]\n",
        "\n",
        "    return values\n",
        "\n",
        "# Apply the function to the 'embedding' column to process embeddings and ensure uniform length\n",
        "embeddings = embeddings_df['embedding'].apply(lambda emb: process_embedding(emb, embedding_length))\n",
        "\n",
        "# Convert the processed embeddings into a PyTorch tensor (Node Feature Matrix)\n",
        "node_feature_matrix = torch.tensor(embeddings.tolist(), dtype=torch.float32)\n",
        "\n",
        "# Print the node feature matrix (with 256 dimensions per node)\n",
        "print(node_feature_matrix)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U55mEBwaH_NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "\n",
        "# Assuming your DataFrame is named 'embeddings_train'\n",
        "node_features = torch.tensor([[float(val) for val in re.findall(r\"[-+]?(?:\\d*\\.\\d+|\\d+)\", emb)]\n",
        "                               for emb in embeddings_train['embedding'].tolist()], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "Rdyyt6x0KOMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import re\n",
        "\n",
        "node_features = torch.tensor([[float(val) for val in re.findall(r\"[-+]?(?:\\d*\\.\\d+|\\d+)\", emb)]\n",
        "                               for emb in embeddings_train['embedding'].tolist()], dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "wc-eXLdxKbg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_features = torch.tensor([eval(emb) for emb in embeddings_train['embedding'].tolist()], dtype=torch.float32)\n",
        "\n",
        "node_features = torch.tensor([eval(emb) for emb in embeddings_train['embedding'].tolist()], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "hZtYEdbYHkvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: based on the data from embeddings_train which contains entities from a knowledge graph and its embeddings,  creta a code for a gnn for text classification\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Assuming 'embeddings_train' DataFrame is loaded and contains 'node' and 'embedding' columns\n",
        "\n",
        "# Create a node feature matrix from the embeddings\n",
        "node_features = torch.tensor([eval(emb) for emb in embeddings_train['embedding'].tolist()], dtype=torch.float32)\n",
        "\n",
        "# Create an adjacency matrix (simplified example, assuming an undirected graph)\n",
        "# You'll need to adapt this based on your actual graph structure\n",
        "adjacency_matrix = torch.zeros((len(embeddings_train), len(embeddings_train)), dtype=torch.float32)\n",
        "for _, row in final_ke_df.iterrows():  # Assuming final_ke_df contains your graph edges\n",
        "  source_node = embeddings_train[embeddings_train['node'] == row['head']].index[0]\n",
        "  target_node = embeddings_train[embeddings_train['node'] == row['tail']].index[0]\n",
        "  adjacency_matrix[source_node, target_node] = 1\n",
        "  adjacency_matrix[target_node, source_node] = 1\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "data = Data(x=node_features, edge_index=adjacency_matrix.nonzero().t().contiguous())\n",
        "\n",
        "# Define the GNN model\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Create an instance of the GNN model\n",
        "model = GNN(input_dim=len(node_features[0]), hidden_dim=64, output_dim=len(label_list))  # Replace with your label list size\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# # Training loop (example)\n",
        "# for epoch in range(100):\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(data)\n",
        "#     loss = criterion(out, labels)  # Replace with your target labels\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     print(f'Epoch: {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "KjbaOKOeDtlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each point on the resulting scatter plot corresponds to a node in the graph."
      ],
      "metadata": {
        "id": "JSG_nSwW9LXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Visualize node embeddings using t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Get embeddings for all nodes\n",
        "embeddings = np.array([model.wv[node] for node in g.nodes()])\n",
        "\n",
        "# Reduce dimensionality using t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=10, n_iter=400)\n",
        "embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Visualize embeddings in 2D space with node labels\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c='blue', alpha=0.7)\n",
        "\n",
        "# Add node labels\n",
        "for i, node in enumerate(g.nodes()):\n",
        "  plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1], node, fontsize=8)\n",
        "  plt.title('Node Embeddings Visualization')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "AGhVptrq8dJV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTAbfspcXQQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step: Initializing the node features: Assign embeddings to each entity (node)."
      ],
      "metadata": {
        "id": "qObeq3nlXpuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node2vec = Node2Vec(g, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
        "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "\n",
        "# Get embeddings for all nodes\n",
        "embeddings = np.array([model.wv[str(node)] for node in g.nodes()])\n",
        "\n",
        "# Apply t-SNE to reduce dimensionality for visualization\n",
        "tsne = TSNE(n_components=2, random_state=42)  # You can adjust the random state\n",
        "embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the embeddings\n",
        "plt.figure(figsize=(10, 8))  # Adjust figure size if needed\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
        "plt.title('Node Embeddings Visualization with t-SNE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BEwdFNye9iLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unjil7vg9m7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in result:\n",
        "  for head, relation, tail in r:\n",
        "    graph.add_edge(head, tail, relation=relation)"
      ],
      "metadata": {
        "id": "k3gpaBSZlLDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pos = nx.spring_layout(graph, k=0.2)\n",
        "nx.draw(graph, pos, with_labels=True, node_size=2000, node_color=\"skyblue\")\n",
        "edge_labels = nx.get_edge_attributes(graph, \"relation\")\n",
        "nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=6, label_pos=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "677WErUzaAyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwO4OKeg7fuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNOv3gHP6v2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "szWByyDq71Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: extrair entitues pairs and relationships\n",
        "# build kg"
      ],
      "metadata": {
        "id": "k141gx_I37U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26DGnsJf3KYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "tJNKOEWWzW2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Graph Neural Network (GNN) Setup\n",
        "\n",
        "Goal: propagate the features through the graph using Graph Convolutional Networks (GCNs) or Graph Attention Networks (GATs)\n",
        "\n",
        "- GNNs will learn to aggregate information from neighboring nodes through the graph structure.\n",
        "- Relationships (edges) between the nodes help in learning context around entities.\n",
        "\n",
        "\n",
        "\n",
        "**Next steps:**\n",
        "- define the approach like GCN or GAT\n",
        "- propagate node features through their neighbors in each layer. After several\n",
        "propagation steps, nodes will have a better understanding of the entire graph's structure and semantics.\n",
        "- after propagating through the GNN, use the final node representations to classify the text based on the aggregated information.\n"
      ],
      "metadata": {
        "id": "Kpe-vibQXUoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Input Layer: The node embeddings\n",
        "- Graph Convolution Layers: These layers will propagate and aggregate the features of neighboring nodes.\n",
        "-\n",
        "Classification Layer: After several layers of graph convolutions, a final layer will classify the node (or graph) into the desired category.\n",
        "\n"
      ],
      "metadata": {
        "id": "FfviwiFuYdw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_loss_values(log_string):\n",
        "    loss_pattern = r\"Loss:\\s*([0-9]*\\.?[0-9]+)\"\n",
        "\n",
        "    loss_values = re.findall(loss_pattern, log_string)\n",
        "\n",
        "    return [float(loss) for loss in loss_values]\n",
        "\n"
      ],
      "metadata": {
        "id": "aVKKHvJHOdHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values_exp1 = \"\"\"Epoch 1, Loss: 1.3868\n",
        "Epoch 2, Loss: 1.3613\n",
        "Epoch 3, Loss: 1.3206\n",
        "Epoch 4, Loss: 1.2711\n",
        "Epoch 5, Loss: 1.2139\n",
        "Epoch 6, Loss: 1.1489\n",
        "Epoch 7, Loss: 1.0842\n",
        "Epoch 8, Loss: 1.0140\n",
        "Epoch 9, Loss: 0.9410\n",
        "Epoch 10, Loss: 0.8742\n",
        "Epoch 11, Loss: 0.7950\n",
        "Epoch 12, Loss: 0.7284\n",
        "Epoch 13, Loss: 0.6630\n",
        "Epoch 14, Loss: 0.5970\n",
        "Epoch 15, Loss: 0.5448\n",
        "Epoch 16, Loss: 0.4901\n",
        "Epoch 17, Loss: 0.4406\n",
        "Epoch 18, Loss: 0.3926\n",
        "Epoch 19, Loss: 0.3519\n",
        "Epoch 20, Loss: 0.3293\n",
        "Epoch 21, Loss: 0.2878\n",
        "Epoch 22, Loss: 0.2683\n",
        "Epoch 23, Loss: 0.2481\n",
        "Epoch 24, Loss: 0.2284\n",
        "Epoch 25, Loss: 0.2075\n",
        "Epoch 26, Loss: 0.1981\n",
        "Epoch 27, Loss: 0.1800\n",
        "Epoch 28, Loss: 0.1708\n",
        "Epoch 29, Loss: 0.1683\n",
        "Epoch 30, Loss: 0.1535\n",
        "Epoch 31, Loss: 0.1441\n",
        "Epoch 32, Loss: 0.1369\n",
        "Epoch 33, Loss: 0.1381\n",
        "Epoch 34, Loss: 0.1304\n",
        "Epoch 35, Loss: 0.1232\n",
        "Epoch 36, Loss: 0.1192\n",
        "Epoch 37, Loss: 0.1130\n",
        "Epoch 38, Loss: 0.1062\n",
        "Epoch 39, Loss: 0.1030\n",
        "Epoch 40, Loss: 0.1065\n",
        "Epoch 41, Loss: 0.1000\n",
        "Epoch 42, Loss: 0.1008\n",
        "Epoch 43, Loss: 0.0980\n",
        "Epoch 44, Loss: 0.0953\n",
        "Epoch 45, Loss: 0.0904\n",
        "Epoch 46, Loss: 0.0856\n",
        "Epoch 47, Loss: 0.0885\n",
        "Epoch 48, Loss: 0.0919\n",
        "Epoch 49, Loss: 0.0827\n",
        "Epoch 50, Loss: 0.0858\n",
        "Epoch 51, Loss: 0.0794\n",
        "Epoch 52, Loss: 0.0822\n",
        "Epoch 53, Loss: 0.0770\n",
        "Epoch 54, Loss: 0.0711\n",
        "Epoch 55, Loss: 0.0781\n",
        "Epoch 56, Loss: 0.0757\n",
        "Epoch 57, Loss: 0.0778\n",
        "Epoch 58, Loss: 0.0729\n",
        "Epoch 59, Loss: 0.0762\n",
        "Epoch 60, Loss: 0.0746\n",
        "Epoch 61, Loss: 0.0659\n",
        "Epoch 62, Loss: 0.0691\n",
        "Epoch 63, Loss: 0.0740\n",
        "Epoch 64, Loss: 0.0698\n",
        "Epoch 65, Loss: 0.0682\n",
        "Epoch 66, Loss: 0.0673\n",
        "Epoch 67, Loss: 0.0694\n",
        "Epoch 68, Loss: 0.0711\n",
        "Epoch 69, Loss: 0.0695\n",
        "Epoch 70, Loss: 0.0643\n",
        "Epoch 71, Loss: 0.0662\n",
        "Epoch 72, Loss: 0.0610\n",
        "Epoch 73, Loss: 0.0611\n",
        "Epoch 74, Loss: 0.0591\n",
        "Epoch 75, Loss: 0.0597\n",
        "Epoch 76, Loss: 0.0554\n",
        "Epoch 77, Loss: 0.0578\n",
        "Epoch 78, Loss: 0.0587\n",
        "Epoch 79, Loss: 0.0546\n",
        "Epoch 80, Loss: 0.0590\n",
        "Epoch 81, Loss: 0.0587\n",
        "Epoch 82, Loss: 0.0570\n",
        "Epoch 83, Loss: 0.0595\n",
        "Epoch 84, Loss: 0.0517\n",
        "Epoch 85, Loss: 0.0529\n",
        "Epoch 86, Loss: 0.0539\n",
        "Epoch 87, Loss: 0.0565\n",
        "Epoch 88, Loss: 0.0559\n",
        "Epoch 89, Loss: 0.0535\n",
        "Epoch 90, Loss: 0.0553\n",
        "Epoch 91, Loss: 0.0556\n",
        "Epoch 92, Loss: 0.0484\n",
        "Epoch 93, Loss: 0.0528\n",
        "Epoch 94, Loss: 0.0505\n",
        "Epoch 95, Loss: 0.0462\n",
        "Epoch 96, Loss: 0.0529\n",
        "Epoch 97, Loss: 0.0520\n",
        "Epoch 98, Loss: 0.0543\n",
        "Epoch 99, Loss: 0.0518\n",
        "Epoch 100, Loss: 0.0514\n",
        "Epoch 101, Loss: 0.0465\n",
        "Epoch 102, Loss: 0.0562\n",
        "Epoch 103, Loss: 0.0549\n",
        "Epoch 104, Loss: 0.0490\n",
        "Epoch 105, Loss: 0.0497\n",
        "Epoch 106, Loss: 0.0500\n",
        "Epoch 107, Loss: 0.0523\n",
        "Epoch 108, Loss: 0.0501\n",
        "Epoch 109, Loss: 0.0468\n",
        "Epoch 110, Loss: 0.0492\n",
        "Epoch 111, Loss: 0.0501\n",
        "Epoch 112, Loss: 0.0532\n",
        "Epoch 113, Loss: 0.0504\n",
        "Epoch 114, Loss: 0.0442\n",
        "Epoch 115, Loss: 0.0467\n",
        "Epoch 116, Loss: 0.0433\n",
        "Epoch 117, Loss: 0.0487\n",
        "Epoch 118, Loss: 0.0447\n",
        "Epoch 119, Loss: 0.0454\n",
        "Epoch 120, Loss: 0.0459\n",
        "Epoch 121, Loss: 0.0434\n",
        "Epoch 122, Loss: 0.0430\n",
        "Epoch 123, Loss: 0.0456\n",
        "Epoch 124, Loss: 0.0430\n",
        "Epoch 125, Loss: 0.0436\n",
        "Epoch 126, Loss: 0.0433\n",
        "Epoch 127, Loss: 0.0470\n",
        "Epoch 128, Loss: 0.0471\n",
        "Epoch 129, Loss: 0.0425\n",
        "Epoch 130, Loss: 0.0395\n",
        "Epoch 131, Loss: 0.0457\n",
        "Epoch 132, Loss: 0.0482\n",
        "Epoch 133, Loss: 0.0453\n",
        "Epoch 134, Loss: 0.0405\n",
        "Epoch 135, Loss: 0.0431\n",
        "Epoch 136, Loss: 0.0408\n",
        "Epoch 137, Loss: 0.0405\n",
        "Epoch 138, Loss: 0.0416\n",
        "Epoch 139, Loss: 0.0421\n",
        "Epoch 140, Loss: 0.0448\n",
        "Epoch 141, Loss: 0.0450\n",
        "Epoch 142, Loss: 0.0439\n",
        "Epoch 143, Loss: 0.0409\n",
        "Epoch 144, Loss: 0.0366\n",
        "Epoch 145, Loss: 0.0411\n",
        "Epoch 146, Loss: 0.0373\n",
        "Epoch 147, Loss: 0.0425\n",
        "Epoch 148, Loss: 0.0451\n",
        "Epoch 149, Loss: 0.0398\n",
        "Epoch 150, Loss: 0.0406\n",
        "Epoch 151, Loss: 0.0421\n",
        "Epoch 152, Loss: 0.0387\n",
        "Epoch 153, Loss: 0.0407\n",
        "Epoch 154, Loss: 0.0392\n",
        "Epoch 155, Loss: 0.0421\n",
        "Epoch 156, Loss: 0.0387\n",
        "Epoch 157, Loss: 0.0387\n",
        "Epoch 158, Loss: 0.0415\n",
        "Epoch 159, Loss: 0.0401\n",
        "Epoch 160, Loss: 0.0371\n",
        "Epoch 161, Loss: 0.0392\n",
        "Epoch 162, Loss: 0.0415\n",
        "Epoch 163, Loss: 0.0379\n",
        "Epoch 164, Loss: 0.0348\n",
        "Epoch 165, Loss: 0.0403\n",
        "Epoch 166, Loss: 0.0324\n",
        "Epoch 167, Loss: 0.0418\n",
        "Epoch 168, Loss: 0.0401\n",
        "Epoch 169, Loss: 0.0404\n",
        "Epoch 170, Loss: 0.0385\n",
        "Epoch 171, Loss: 0.0368\n",
        "Epoch 172, Loss: 0.0379\n",
        "Epoch 173, Loss: 0.0359\n",
        "Epoch 174, Loss: 0.0415\n",
        "Epoch 175, Loss: 0.0359\n",
        "Epoch 176, Loss: 0.0363\n",
        "Epoch 177, Loss: 0.0375\n",
        "Epoch 178, Loss: 0.0412\n",
        "Epoch 179, Loss: 0.0330\n",
        "Epoch 180, Loss: 0.0360\n",
        "Epoch 181, Loss: 0.0359\n",
        "Epoch 182, Loss: 0.0353\n",
        "Epoch 183, Loss: 0.0345\n",
        "Epoch 184, Loss: 0.0390\n",
        "Epoch 185, Loss: 0.0367\n",
        "Epoch 186, Loss: 0.0309\n",
        "Epoch 187, Loss: 0.0405\n",
        "Epoch 188, Loss: 0.0382\n",
        "Epoch 189, Loss: 0.0386\n",
        "Epoch 190, Loss: 0.0367\n",
        "Epoch 191, Loss: 0.0376\n",
        "Epoch 192, Loss: 0.0363\n",
        "Epoch 193, Loss: 0.0361\n",
        "Epoch 194, Loss: 0.0373\n",
        "Epoch 195, Loss: 0.0370\n",
        "Epoch 196, Loss: 0.0320\n",
        "Epoch 197, Loss: 0.0343\n",
        "Epoch 198, Loss: 0.0366\n",
        "Epoch 199, Loss: 0.0362\n",
        "Epoch 200, Loss: 0.0369\n",
        "Epoch 201, Loss: 0.0345\n",
        "Epoch 202, Loss: 0.0341\n",
        "Epoch 203, Loss: 0.0318\n",
        "Epoch 204, Loss: 0.0344\n",
        "Epoch 205, Loss: 0.0345\n",
        "Epoch 206, Loss: 0.0367\n",
        "Epoch 207, Loss: 0.0324\n",
        "Epoch 208, Loss: 0.0334\n",
        "Epoch 209, Loss: 0.0353\n",
        "Epoch 210, Loss: 0.0340\n",
        "Epoch 211, Loss: 0.0363\n",
        "Epoch 212, Loss: 0.0379\n",
        "Epoch 213, Loss: 0.0328\n",
        "Epoch 214, Loss: 0.0357\n",
        "Epoch 215, Loss: 0.0348\n",
        "Epoch 216, Loss: 0.0340\n",
        "Epoch 217, Loss: 0.0315\n",
        "Epoch 218, Loss: 0.0329\n",
        "Epoch 219, Loss: 0.0343\n",
        "Epoch 220, Loss: 0.0332\n",
        "Epoch 221, Loss: 0.0380\n",
        "Epoch 222, Loss: 0.0328\n",
        "Epoch 223, Loss: 0.0369\n",
        "Epoch 224, Loss: 0.0330\n",
        "Epoch 225, Loss: 0.0328\n",
        "Epoch 226, Loss: 0.0327\n",
        "Epoch 227, Loss: 0.0323\n",
        "Epoch 228, Loss: 0.0324\n",
        "Epoch 229, Loss: 0.0347\n",
        "Epoch 230, Loss: 0.0328\n",
        "Epoch 231, Loss: 0.0372\n",
        "Epoch 232, Loss: 0.0299\n",
        "Epoch 233, Loss: 0.0339\n",
        "Epoch 234, Loss: 0.0353\n",
        "Epoch 235, Loss: 0.0333\n",
        "Epoch 236, Loss: 0.0388\n",
        "Epoch 237, Loss: 0.0300\n",
        "Epoch 238, Loss: 0.0324\n",
        "Epoch 239, Loss: 0.0349\n",
        "Epoch 240, Loss: 0.0313\n",
        "Epoch 241, Loss: 0.0277\n",
        "Epoch 242, Loss: 0.0293\n",
        "Epoch 243, Loss: 0.0305\n",
        "Epoch 244, Loss: 0.0301\n",
        "Epoch 245, Loss: 0.0359\n",
        "Epoch 246, Loss: 0.0329\n",
        "Epoch 247, Loss: 0.0330\n",
        "Epoch 248, Loss: 0.0323\n",
        "Epoch 249, Loss: 0.0306\n",
        "Epoch 250, Loss: 0.0363\n",
        "Epoch 251, Loss: 0.0330\n",
        "Epoch 252, Loss: 0.0328\n",
        "Epoch 253, Loss: 0.0303\n",
        "Epoch 254, Loss: 0.0324\n",
        "Epoch 255, Loss: 0.0321\n",
        "Epoch 256, Loss: 0.0330\n",
        "Epoch 257, Loss: 0.0309\n",
        "Epoch 258, Loss: 0.0348\n",
        "Epoch 259, Loss: 0.0309\n",
        "Epoch 260, Loss: 0.0281\n",
        "Epoch 261, Loss: 0.0319\n",
        "Epoch 262, Loss: 0.0331\n",
        "Epoch 263, Loss: 0.0333\n",
        "Epoch 264, Loss: 0.0336\n",
        "Epoch 265, Loss: 0.0335\n",
        "Epoch 266, Loss: 0.0285\n",
        "Epoch 267, Loss: 0.0314\n",
        "Epoch 268, Loss: 0.0300\n",
        "Epoch 269, Loss: 0.0320\n",
        "Epoch 270, Loss: 0.0307\n",
        "Epoch 271, Loss: 0.0293\n",
        "Epoch 272, Loss: 0.0342\n",
        "Epoch 273, Loss: 0.0301\n",
        "Epoch 274, Loss: 0.0301\n",
        "Epoch 275, Loss: 0.0356\n",
        "Epoch 276, Loss: 0.0307\n",
        "Epoch 277, Loss: 0.0301\n",
        "Epoch 278, Loss: 0.0303\n",
        "Epoch 279, Loss: 0.0337\n",
        "Epoch 280, Loss: 0.0322\n",
        "Epoch 281, Loss: 0.0301\n",
        "Epoch 282, Loss: 0.0321\n",
        "Epoch 283, Loss: 0.0317\n",
        "Epoch 284, Loss: 0.0295\n",
        "Epoch 285, Loss: 0.0300\n",
        "Epoch 286, Loss: 0.0304\n",
        "Epoch 287, Loss: 0.0292\n",
        "Epoch 288, Loss: 0.0297\n",
        "Epoch 289, Loss: 0.0313\n",
        "Epoch 290, Loss: 0.0291\n",
        "Epoch 291, Loss: 0.0319\n",
        "Epoch 292, Loss: 0.0334\n",
        "Epoch 293, Loss: 0.0306\n",
        "Epoch 294, Loss: 0.0286\n",
        "Epoch 295, Loss: 0.0298\n",
        "Epoch 296, Loss: 0.0346\n",
        "Epoch 297, Loss: 0.0297\n",
        "Epoch 298, Loss: 0.0319\n",
        "Epoch 299, Loss: 0.0343\n",
        "Epoch 300, Loss: 0.0303\n",
        "Epoch 301, Loss: 0.0291\n",
        "Epoch 302, Loss: 0.0297\n",
        "Epoch 303, Loss: 0.0293\n",
        "Epoch 304, Loss: 0.0292\n",
        "Epoch 305, Loss: 0.0303\n",
        "Epoch 306, Loss: 0.0311\n",
        "Epoch 307, Loss: 0.0271\n",
        "Epoch 308, Loss: 0.0255\n",
        "Epoch 309, Loss: 0.0335\n",
        "Epoch 310, Loss: 0.0314\n",
        "Epoch 311, Loss: 0.0317\n",
        "Epoch 312, Loss: 0.0287\n",
        "Epoch 313, Loss: 0.0280\n",
        "Epoch 314, Loss: 0.0317\n",
        "Epoch 315, Loss: 0.0301\n",
        "Epoch 316, Loss: 0.0333\n",
        "Epoch 317, Loss: 0.0310\n",
        "Epoch 318, Loss: 0.0342\n",
        "Epoch 319, Loss: 0.0320\n",
        "Epoch 320, Loss: 0.0311\n",
        "Epoch 321, Loss: 0.0314\n",
        "Epoch 322, Loss: 0.0278\n",
        "Epoch 323, Loss: 0.0323\n",
        "Epoch 324, Loss: 0.0284\n",
        "Epoch 325, Loss: 0.0352\n",
        "Epoch 326, Loss: 0.0303\n",
        "Epoch 327, Loss: 0.0263\n",
        "Epoch 328, Loss: 0.0316\n",
        "Epoch 329, Loss: 0.0282\n",
        "Epoch 330, Loss: 0.0299\n",
        "Epoch 331, Loss: 0.0281\n",
        "Epoch 332, Loss: 0.0312\n",
        "Epoch 333, Loss: 0.0318\n",
        "Epoch 334, Loss: 0.0275\n",
        "Epoch 335, Loss: 0.0315\n",
        "Epoch 336, Loss: 0.0285\n",
        "Epoch 337, Loss: 0.0253\n",
        "Epoch 338, Loss: 0.0302\n",
        "Epoch 339, Loss: 0.0301\n",
        "Epoch 340, Loss: 0.0306\n",
        "Epoch 341, Loss: 0.0273\n",
        "Epoch 342, Loss: 0.0280\n",
        "Epoch 343, Loss: 0.0339\n",
        "Epoch 344, Loss: 0.0276\n",
        "Epoch 345, Loss: 0.0292\n",
        "Epoch 346, Loss: 0.0312\n",
        "Epoch 347, Loss: 0.0353\n",
        "Epoch 348, Loss: 0.0288\n",
        "Epoch 349, Loss: 0.0268\n",
        "Epoch 350, Loss: 0.0305\n",
        "Epoch 351, Loss: 0.0259\n",
        "Epoch 352, Loss: 0.0301\n",
        "Epoch 353, Loss: 0.0292\n",
        "Epoch 354, Loss: 0.0297\n",
        "Epoch 355, Loss: 0.0289\n",
        "Epoch 356, Loss: 0.0288\n",
        "Epoch 357, Loss: 0.0253\n",
        "Epoch 358, Loss: 0.0285\n",
        "Epoch 359, Loss: 0.0281\n",
        "Epoch 360, Loss: 0.0302\n",
        "Epoch 361, Loss: 0.0276\n",
        "Epoch 362, Loss: 0.0276\n",
        "Epoch 363, Loss: 0.0273\n",
        "Epoch 364, Loss: 0.0311\n",
        "Epoch 365, Loss: 0.0279\n",
        "Epoch 366, Loss: 0.0313\n",
        "Epoch 367, Loss: 0.0304\n",
        "Epoch 368, Loss: 0.0276\n",
        "Epoch 369, Loss: 0.0261\n",
        "Epoch 370, Loss: 0.0299\n",
        "Epoch 371, Loss: 0.0322\n",
        "Epoch 372, Loss: 0.0255\n",
        "Epoch 373, Loss: 0.0268\n",
        "Epoch 374, Loss: 0.0307\n",
        "Epoch 375, Loss: 0.0313\n",
        "Epoch 376, Loss: 0.0277\n",
        "Epoch 377, Loss: 0.0289\n",
        "Epoch 378, Loss: 0.0304\n",
        "Epoch 379, Loss: 0.0312\n",
        "Epoch 380, Loss: 0.0315\n",
        "Epoch 381, Loss: 0.0272\n",
        "Epoch 382, Loss: 0.0299\n",
        "Epoch 383, Loss: 0.0268\n",
        "Epoch 384, Loss: 0.0282\n",
        "Epoch 385, Loss: 0.0316\n",
        "Epoch 386, Loss: 0.0281\n",
        "Epoch 387, Loss: 0.0311\n",
        "Epoch 388, Loss: 0.0295\n",
        "Epoch 389, Loss: 0.0302\n",
        "Epoch 390, Loss: 0.0302\n",
        "Epoch 391, Loss: 0.0282\n",
        "Epoch 392, Loss: 0.0250\n",
        "Epoch 393, Loss: 0.0289\n",
        "Epoch 394, Loss: 0.0301\n",
        "Epoch 395, Loss: 0.0257\n",
        "Epoch 396, Loss: 0.0294\n",
        "Epoch 397, Loss: 0.0268\n",
        "Epoch 398, Loss: 0.0264\n",
        "Epoch 399, Loss: 0.0276\n",
        "Epoch 400, Loss: 0.0223\n",
        "Epoch 401, Loss: 0.0289\n",
        "Epoch 402, Loss: 0.0265\n",
        "Epoch 403, Loss: 0.0284\n",
        "Epoch 404, Loss: 0.0283\n",
        "Epoch 405, Loss: 0.0271\n",
        "Epoch 406, Loss: 0.0278\n",
        "Epoch 407, Loss: 0.0267\n",
        "Epoch 408, Loss: 0.0279\n",
        "Epoch 409, Loss: 0.0298\n",
        "Epoch 410, Loss: 0.0255\n",
        "Epoch 411, Loss: 0.0283\n",
        "Epoch 412, Loss: 0.0299\n",
        "Epoch 413, Loss: 0.0269\n",
        "Epoch 414, Loss: 0.0251\n",
        "Epoch 415, Loss: 0.0293\n",
        "Epoch 416, Loss: 0.0248\n",
        "Epoch 417, Loss: 0.0280\n",
        "Epoch 418, Loss: 0.0275\n",
        "Epoch 419, Loss: 0.0297\n",
        "Epoch 420, Loss: 0.0255\n",
        "Epoch 421, Loss: 0.0260\n",
        "Epoch 422, Loss: 0.0297\n",
        "Epoch 423, Loss: 0.0227\n",
        "Epoch 424, Loss: 0.0317\n",
        "Epoch 425, Loss: 0.0300\n",
        "Epoch 426, Loss: 0.0285\n",
        "Epoch 427, Loss: 0.0273\n",
        "Epoch 428, Loss: 0.0245\n",
        "Epoch 429, Loss: 0.0288\n",
        "Epoch 430, Loss: 0.0251\n",
        "Epoch 431, Loss: 0.0255\n",
        "Epoch 432, Loss: 0.0280\n",
        "Epoch 433, Loss: 0.0264\n",
        "Epoch 434, Loss: 0.0268\n",
        "Epoch 435, Loss: 0.0229\n",
        "Epoch 436, Loss: 0.0293\n",
        "Epoch 437, Loss: 0.0238\n",
        "Epoch 438, Loss: 0.0283\n",
        "Epoch 439, Loss: 0.0240\n",
        "Epoch 440, Loss: 0.0281\n",
        "Epoch 441, Loss: 0.0248\n",
        "Epoch 442, Loss: 0.0281\n",
        "Epoch 443, Loss: 0.0259\n",
        "Epoch 444, Loss: 0.0274\n",
        "Epoch 445, Loss: 0.0251\n",
        "Epoch 446, Loss: 0.0266\n",
        "Epoch 447, Loss: 0.0248\n",
        "Epoch 448, Loss: 0.0307\n",
        "Epoch 449, Loss: 0.0301\n",
        "Epoch 450, Loss: 0.0273\n",
        "Epoch 451, Loss: 0.0255\n",
        "Epoch 452, Loss: 0.0244\n",
        "Epoch 453, Loss: 0.0272\n",
        "Epoch 454, Loss: 0.0280\n",
        "Epoch 455, Loss: 0.0264\n",
        "Epoch 456, Loss: 0.0303\n",
        "Epoch 457, Loss: 0.0279\n",
        "Epoch 458, Loss: 0.0251\n",
        "Epoch 459, Loss: 0.0268\n",
        "Epoch 460, Loss: 0.0232\n",
        "Epoch 461, Loss: 0.0288\n",
        "Epoch 462, Loss: 0.0272\n",
        "Epoch 463, Loss: 0.0290\n",
        "Epoch 464, Loss: 0.0274\n",
        "Epoch 465, Loss: 0.0248\n",
        "Epoch 466, Loss: 0.0258\n",
        "Epoch 467, Loss: 0.0273\n",
        "Epoch 468, Loss: 0.0252\n",
        "Epoch 469, Loss: 0.0280\n",
        "Epoch 470, Loss: 0.0286\n",
        "Epoch 471, Loss: 0.0281\n",
        "Epoch 472, Loss: 0.0290\n",
        "Epoch 473, Loss: 0.0251\n",
        "Epoch 474, Loss: 0.0286\n",
        "Epoch 475, Loss: 0.0273\n",
        "Epoch 476, Loss: 0.0252\n",
        "Epoch 477, Loss: 0.0234\n",
        "Epoch 478, Loss: 0.0264\n",
        "Epoch 479, Loss: 0.0271\n",
        "Epoch 480, Loss: 0.0266\n",
        "Epoch 481, Loss: 0.0331\n",
        "Epoch 482, Loss: 0.0241\n",
        "Epoch 483, Loss: 0.0250\n",
        "Epoch 484, Loss: 0.0278\n",
        "Epoch 485, Loss: 0.0283\n",
        "Epoch 486, Loss: 0.0280\n",
        "Epoch 487, Loss: 0.0249\n",
        "Epoch 488, Loss: 0.0273\n",
        "Epoch 489, Loss: 0.0229\n",
        "Epoch 490, Loss: 0.0286\n",
        "Epoch 491, Loss: 0.0280\n",
        "Epoch 492, Loss: 0.0262\n",
        "Epoch 493, Loss: 0.0244\n",
        "Epoch 494, Loss: 0.0273\n",
        "Epoch 495, Loss: 0.0287\n",
        "Epoch 496, Loss: 0.0245\n",
        "Epoch 497, Loss: 0.0279\n",
        "Epoch 498, Loss: 0.0269\n",
        "Epoch 499, Loss: 0.0279\n",
        "Epoch 500, Loss: 0.0275\n",
        "Epoch 501, Loss: 0.0277\n",
        "Epoch 502, Loss: 0.0252\n",
        "Epoch 503, Loss: 0.0292\n",
        "Epoch 504, Loss: 0.0242\n",
        "Epoch 505, Loss: 0.0290\n",
        "Epoch 506, Loss: 0.0265\n",
        "Epoch 507, Loss: 0.0290\n",
        "Epoch 508, Loss: 0.0247\n",
        "Epoch 509, Loss: 0.0282\n",
        "Epoch 510, Loss: 0.0260\n",
        "Epoch 511, Loss: 0.0234\n",
        "Epoch 512, Loss: 0.0256\n",
        "Epoch 513, Loss: 0.0279\n",
        "Epoch 514, Loss: 0.0272\n",
        "Epoch 515, Loss: 0.0288\n",
        "Epoch 516, Loss: 0.0250\n",
        "Epoch 517, Loss: 0.0282\n",
        "Epoch 518, Loss: 0.0289\n",
        "Epoch 519, Loss: 0.0260\n",
        "Epoch 520, Loss: 0.0284\n",
        "Epoch 521, Loss: 0.0240\n",
        "Epoch 522, Loss: 0.0260\n",
        "Epoch 523, Loss: 0.0296\n",
        "Epoch 524, Loss: 0.0263\n",
        "Epoch 525, Loss: 0.0289\n",
        "Epoch 526, Loss: 0.0263\n",
        "Epoch 527, Loss: 0.0282\n",
        "Epoch 528, Loss: 0.0273\n",
        "Epoch 529, Loss: 0.0307\n",
        "Epoch 530, Loss: 0.0267\n",
        "Epoch 531, Loss: 0.0226\n",
        "Epoch 532, Loss: 0.0264\n",
        "Epoch 533, Loss: 0.0282\n",
        "Epoch 534, Loss: 0.0279\n",
        "Epoch 535, Loss: 0.0286\n",
        "Epoch 536, Loss: 0.0276\n",
        "Epoch 537, Loss: 0.0234\n",
        "Epoch 538, Loss: 0.0255\n",
        "Epoch 539, Loss: 0.0236\n",
        "Epoch 540, Loss: 0.0271\n",
        "Epoch 541, Loss: 0.0252\n",
        "Epoch 542, Loss: 0.0268\n",
        "Epoch 543, Loss: 0.0258\n",
        "Epoch 544, Loss: 0.0280\n",
        "Epoch 545, Loss: 0.0288\n",
        "Epoch 546, Loss: 0.0264\n",
        "Epoch 547, Loss: 0.0274\n",
        "Epoch 548, Loss: 0.0270\n",
        "Epoch 549, Loss: 0.0263\n",
        "Epoch 550, Loss: 0.0258\n",
        "Epoch 551, Loss: 0.0280\n",
        "Epoch 552, Loss: 0.0290\n",
        "Epoch 553, Loss: 0.0257\n",
        "Epoch 554, Loss: 0.0265\n",
        "Epoch 555, Loss: 0.0271\n",
        "Epoch 556, Loss: 0.0240\n",
        "Epoch 557, Loss: 0.0263\n",
        "Epoch 558, Loss: 0.0288\n",
        "Epoch 559, Loss: 0.0266\n",
        "Epoch 560, Loss: 0.0259\n",
        "Epoch 561, Loss: 0.0266\n",
        "Epoch 562, Loss: 0.0280\n",
        "Epoch 563, Loss: 0.0306\n",
        "Epoch 564, Loss: 0.0254\n",
        "Epoch 565, Loss: 0.0269\n",
        "Epoch 566, Loss: 0.0229\n",
        "Epoch 567, Loss: 0.0296\n",
        "Epoch 568, Loss: 0.0317\n",
        "Epoch 569, Loss: 0.0269\n",
        "Epoch 570, Loss: 0.0273\n",
        "Epoch 571, Loss: 0.0261\n",
        "Epoch 572, Loss: 0.0226\n",
        "Epoch 573, Loss: 0.0257\n",
        "Epoch 574, Loss: 0.0264\n",
        "Epoch 575, Loss: 0.0278\n",
        "Epoch 576, Loss: 0.0296\n",
        "Epoch 577, Loss: 0.0258\n",
        "Epoch 578, Loss: 0.0276\n",
        "Epoch 579, Loss: 0.0255\n",
        "Epoch 580, Loss: 0.0269\n",
        "Epoch 581, Loss: 0.0265\n",
        "Epoch 582, Loss: 0.0247\n",
        "Epoch 583, Loss: 0.0260\n",
        "Epoch 584, Loss: 0.0284\n",
        "Epoch 585, Loss: 0.0265\n",
        "Epoch 586, Loss: 0.0273\n",
        "Epoch 587, Loss: 0.0251\n",
        "Epoch 588, Loss: 0.0240\n",
        "Epoch 589, Loss: 0.0254\n",
        "Epoch 590, Loss: 0.0216\n",
        "Epoch 591, Loss: 0.0285\n",
        "Epoch 592, Loss: 0.0245\n",
        "Epoch 593, Loss: 0.0277\n",
        "Epoch 594, Loss: 0.0286\n",
        "Epoch 595, Loss: 0.0258\n",
        "Epoch 596, Loss: 0.0269\n",
        "Epoch 597, Loss: 0.0301\n",
        "Epoch 598, Loss: 0.0251\n",
        "Epoch 599, Loss: 0.0279\n",
        "Epoch 600, Loss: 0.0254\n",
        "Epoch 601, Loss: 0.0243\n",
        "Epoch 602, Loss: 0.0252\n",
        "Epoch 603, Loss: 0.0259\n",
        "Epoch 604, Loss: 0.0247\n",
        "Epoch 605, Loss: 0.0248\n",
        "Epoch 606, Loss: 0.0263\n",
        "Epoch 607, Loss: 0.0287\n",
        "Epoch 608, Loss: 0.0288\n",
        "Epoch 609, Loss: 0.0294\n",
        "Epoch 610, Loss: 0.0254\n",
        "Epoch 611, Loss: 0.0288\n",
        "Epoch 612, Loss: 0.0250\n",
        "Epoch 613, Loss: 0.0282\n",
        "Epoch 614, Loss: 0.0225\n",
        "Epoch 615, Loss: 0.0291\n",
        "Epoch 616, Loss: 0.0279\n",
        "Epoch 617, Loss: 0.0237\n",
        "Epoch 618, Loss: 0.0246\n",
        "Epoch 619, Loss: 0.0256\n",
        "Epoch 620, Loss: 0.0303\n",
        "Epoch 621, Loss: 0.0250\n",
        "Epoch 622, Loss: 0.0281\n",
        "Epoch 623, Loss: 0.0260\n",
        "Epoch 624, Loss: 0.0268\n",
        "Epoch 625, Loss: 0.0263\n",
        "Epoch 626, Loss: 0.0209\n",
        "Epoch 627, Loss: 0.0225\n",
        "Epoch 628, Loss: 0.0258\n",
        "Epoch 629, Loss: 0.0266\n",
        "Epoch 630, Loss: 0.0253\n",
        "Epoch 631, Loss: 0.0289\n",
        "Epoch 632, Loss: 0.0280\n",
        "Epoch 633, Loss: 0.0273\n",
        "Epoch 634, Loss: 0.0244\n",
        "Epoch 635, Loss: 0.0275\n",
        "Epoch 636, Loss: 0.0251\n",
        "Epoch 637, Loss: 0.0219\n",
        "Epoch 638, Loss: 0.0261\n",
        "Epoch 639, Loss: 0.0238\n",
        "Epoch 640, Loss: 0.0309\n",
        "Epoch 641, Loss: 0.0279\n",
        "Epoch 642, Loss: 0.0243\n",
        "Epoch 643, Loss: 0.0254\n",
        "Epoch 644, Loss: 0.0237\n",
        "Epoch 645, Loss: 0.0271\n",
        "Epoch 646, Loss: 0.0251\n",
        "Epoch 647, Loss: 0.0272\n",
        "Epoch 648, Loss: 0.0268\n",
        "Epoch 649, Loss: 0.0270\n",
        "Epoch 650, Loss: 0.0282\n",
        "Epoch 651, Loss: 0.0240\n",
        "Epoch 652, Loss: 0.0244\n",
        "Epoch 653, Loss: 0.0285\n",
        "Epoch 654, Loss: 0.0268\n",
        "Epoch 655, Loss: 0.0255\n",
        "Epoch 656, Loss: 0.0240\n",
        "Epoch 657, Loss: 0.0245\n",
        "Epoch 658, Loss: 0.0269\n",
        "Epoch 659, Loss: 0.0218\n",
        "Epoch 660, Loss: 0.0236\n",
        "Epoch 661, Loss: 0.0258\n",
        "Epoch 662, Loss: 0.0274\n",
        "Epoch 663, Loss: 0.0246\n",
        "Epoch 664, Loss: 0.0250\n",
        "Epoch 665, Loss: 0.0230\n",
        "Epoch 666, Loss: 0.0285\n",
        "Epoch 667, Loss: 0.0278\n",
        "Epoch 668, Loss: 0.0248\n",
        "Epoch 669, Loss: 0.0231\n",
        "Epoch 670, Loss: 0.0269\n",
        "Epoch 671, Loss: 0.0274\n",
        "Epoch 672, Loss: 0.0264\n",
        "Epoch 673, Loss: 0.0229\n",
        "Epoch 674, Loss: 0.0237\n",
        "Epoch 675, Loss: 0.0232\n",
        "Epoch 676, Loss: 0.0221\n",
        "Epoch 677, Loss: 0.0253\n",
        "Epoch 678, Loss: 0.0263\n",
        "Epoch 679, Loss: 0.0221\n",
        "Epoch 680, Loss: 0.0284\n",
        "Epoch 681, Loss: 0.0263\n",
        "Epoch 682, Loss: 0.0268\n",
        "Epoch 683, Loss: 0.0258\n",
        "Epoch 684, Loss: 0.0280\n",
        "Epoch 685, Loss: 0.0249\n",
        "Epoch 686, Loss: 0.0234\n",
        "Epoch 687, Loss: 0.0254\n",
        "Epoch 688, Loss: 0.0233\n",
        "Epoch 689, Loss: 0.0284\n",
        "Epoch 690, Loss: 0.0234\n",
        "Epoch 691, Loss: 0.0263\n",
        "Epoch 692, Loss: 0.0248\n",
        "Epoch 693, Loss: 0.0252\n",
        "Epoch 694, Loss: 0.0264\n",
        "Epoch 695, Loss: 0.0274\n",
        "Epoch 696, Loss: 0.0225\n",
        "Epoch 697, Loss: 0.0241\n",
        "Epoch 698, Loss: 0.0256\n",
        "Epoch 699, Loss: 0.0252\n",
        "Epoch 700, Loss: 0.0242\n",
        "Epoch 701, Loss: 0.0250\n",
        "Epoch 702, Loss: 0.0226\n",
        "Epoch 703, Loss: 0.0286\n",
        "Epoch 704, Loss: 0.0222\n",
        "Epoch 705, Loss: 0.0216\n",
        "Epoch 706, Loss: 0.0267\n",
        "Epoch 707, Loss: 0.0257\n",
        "Epoch 708, Loss: 0.0230\n",
        "Epoch 709, Loss: 0.0277\n",
        "Epoch 710, Loss: 0.0236\n",
        "Epoch 711, Loss: 0.0222\n",
        "Epoch 712, Loss: 0.0242\n",
        "Epoch 713, Loss: 0.0225\n",
        "Epoch 714, Loss: 0.0245\n",
        "Epoch 715, Loss: 0.0216\n",
        "Epoch 716, Loss: 0.0293\n",
        "Epoch 717, Loss: 0.0265\n",
        "Epoch 718, Loss: 0.0248\n",
        "Epoch 719, Loss: 0.0259\n",
        "Epoch 720, Loss: 0.0272\n",
        "Epoch 721, Loss: 0.0248\n",
        "Epoch 722, Loss: 0.0255\n",
        "Epoch 723, Loss: 0.0219\n",
        "Epoch 724, Loss: 0.0270\n",
        "Epoch 725, Loss: 0.0237\n",
        "Epoch 726, Loss: 0.0248\n",
        "Epoch 727, Loss: 0.0241\n",
        "Epoch 728, Loss: 0.0235\n",
        "Epoch 729, Loss: 0.0282\n",
        "Epoch 730, Loss: 0.0233\n",
        "Epoch 731, Loss: 0.0264\n",
        "Epoch 732, Loss: 0.0270\n",
        "Epoch 733, Loss: 0.0223\n",
        "Epoch 734, Loss: 0.0278\n",
        "Epoch 735, Loss: 0.0251\n",
        "Epoch 736, Loss: 0.0236\n",
        "Epoch 737, Loss: 0.0214\n",
        "Epoch 738, Loss: 0.0221\n",
        "Epoch 739, Loss: 0.0244\n",
        "Epoch 740, Loss: 0.0261\n",
        "Epoch 741, Loss: 0.0232\n",
        "Epoch 742, Loss: 0.0237\n",
        "Epoch 743, Loss: 0.0240\n",
        "Epoch 744, Loss: 0.0251\n",
        "Epoch 745, Loss: 0.0225\n",
        "Epoch 746, Loss: 0.0240\n",
        "Epoch 747, Loss: 0.0253\n",
        "Epoch 748, Loss: 0.0241\n",
        "Epoch 749, Loss: 0.0263\n",
        "Epoch 750, Loss: 0.0254\n",
        "Epoch 751, Loss: 0.0260\n",
        "Epoch 752, Loss: 0.0259\n",
        "Epoch 753, Loss: 0.0218\n",
        "Epoch 754, Loss: 0.0291\n",
        "Epoch 755, Loss: 0.0274\n",
        "Epoch 756, Loss: 0.0241\n",
        "Epoch 757, Loss: 0.0250\n",
        "Epoch 758, Loss: 0.0238\n",
        "Epoch 759, Loss: 0.0218\n",
        "Epoch 760, Loss: 0.0237\n",
        "Epoch 761, Loss: 0.0197\n",
        "Epoch 762, Loss: 0.0242\n",
        "Epoch 763, Loss: 0.0215\n",
        "Epoch 764, Loss: 0.0209\n",
        "Epoch 765, Loss: 0.0233\n",
        "Epoch 766, Loss: 0.0229\n",
        "Epoch 767, Loss: 0.0247\n",
        "Epoch 768, Loss: 0.0239\n",
        "Epoch 769, Loss: 0.0245\n",
        "Epoch 770, Loss: 0.0260\n",
        "Epoch 771, Loss: 0.0219\n",
        "Epoch 772, Loss: 0.0221\n",
        "Epoch 773, Loss: 0.0268\n",
        "Epoch 774, Loss: 0.0232\n",
        "Epoch 775, Loss: 0.0204\n",
        "Epoch 776, Loss: 0.0291\n",
        "Epoch 777, Loss: 0.0250\n",
        "Epoch 778, Loss: 0.0267\n",
        "Epoch 779, Loss: 0.0206\n",
        "Epoch 780, Loss: 0.0232\n",
        "Epoch 781, Loss: 0.0255\n",
        "Epoch 782, Loss: 0.0270\n",
        "Epoch 783, Loss: 0.0266\n",
        "Epoch 784, Loss: 0.0261\n",
        "Epoch 785, Loss: 0.0250\n",
        "Epoch 786, Loss: 0.0242\n",
        "Epoch 787, Loss: 0.0273\n",
        "Epoch 788, Loss: 0.0259\n",
        "Epoch 789, Loss: 0.0233\n",
        "Epoch 790, Loss: 0.0287\n",
        "Epoch 791, Loss: 0.0225\n",
        "Epoch 792, Loss: 0.0231\n",
        "Epoch 793, Loss: 0.0248\n",
        "Epoch 794, Loss: 0.0244\n",
        "Epoch 795, Loss: 0.0233\n",
        "Epoch 796, Loss: 0.0254\n",
        "Epoch 797, Loss: 0.0250\n",
        "Epoch 798, Loss: 0.0256\n",
        "Epoch 799, Loss: 0.0274\n",
        "Epoch 800, Loss: 0.0222\n",
        "Epoch 801, Loss: 0.0219\n",
        "Epoch 802, Loss: 0.0263\n",
        "Epoch 803, Loss: 0.0235\n",
        "Epoch 804, Loss: 0.0236\n",
        "Epoch 805, Loss: 0.0229\n",
        "Epoch 806, Loss: 0.0271\n",
        "Epoch 807, Loss: 0.0198\n",
        "Epoch 808, Loss: 0.0241\n",
        "Epoch 809, Loss: 0.0227\n",
        "Epoch 810, Loss: 0.0273\n",
        "Epoch 811, Loss: 0.0229\n",
        "Epoch 812, Loss: 0.0220\n",
        "Epoch 813, Loss: 0.0275\n",
        "Epoch 814, Loss: 0.0247\n",
        "Epoch 815, Loss: 0.0246\n",
        "Epoch 816, Loss: 0.0272\n",
        "Epoch 817, Loss: 0.0254\n",
        "Epoch 818, Loss: 0.0262\n",
        "Epoch 819, Loss: 0.0247\n",
        "Epoch 820, Loss: 0.0264\n",
        "Epoch 821, Loss: 0.0255\n",
        "Epoch 822, Loss: 0.0247\n",
        "Epoch 823, Loss: 0.0244\n",
        "Epoch 824, Loss: 0.0223\n",
        "Epoch 825, Loss: 0.0248\n",
        "Epoch 826, Loss: 0.0285\n",
        "Epoch 827, Loss: 0.0255\n",
        "Epoch 828, Loss: 0.0242\n",
        "Epoch 829, Loss: 0.0222\n",
        "Epoch 830, Loss: 0.0250\n",
        "Epoch 831, Loss: 0.0225\n",
        "Epoch 832, Loss: 0.0202\n",
        "Epoch 833, Loss: 0.0238\n",
        "Epoch 834, Loss: 0.0195\n",
        "Epoch 835, Loss: 0.0215\n",
        "Epoch 836, Loss: 0.0242\n",
        "Epoch 837, Loss: 0.0263\n",
        "Epoch 838, Loss: 0.0271\n",
        "Epoch 839, Loss: 0.0240\n",
        "Epoch 840, Loss: 0.0249\n",
        "Epoch 841, Loss: 0.0275\n",
        "Epoch 842, Loss: 0.0265\n",
        "Epoch 843, Loss: 0.0217\n",
        "Epoch 844, Loss: 0.0243\n",
        "Epoch 845, Loss: 0.0212\n",
        "Epoch 846, Loss: 0.0251\n",
        "Epoch 847, Loss: 0.0219\n",
        "Epoch 848, Loss: 0.0249\n",
        "Epoch 849, Loss: 0.0225\n",
        "Epoch 850, Loss: 0.0223\n",
        "Epoch 851, Loss: 0.0269\n",
        "Epoch 852, Loss: 0.0267\n",
        "Epoch 853, Loss: 0.0216\n",
        "Epoch 854, Loss: 0.0225\n",
        "Epoch 855, Loss: 0.0222\n",
        "Epoch 856, Loss: 0.0233\n",
        "Epoch 857, Loss: 0.0260\n",
        "Epoch 858, Loss: 0.0265\n",
        "Epoch 859, Loss: 0.0218\n",
        "Epoch 860, Loss: 0.0275\n",
        "Epoch 861, Loss: 0.0235\n",
        "Epoch 862, Loss: 0.0251\n",
        "Epoch 863, Loss: 0.0244\n",
        "Epoch 864, Loss: 0.0265\n",
        "Epoch 865, Loss: 0.0224\n",
        "Epoch 866, Loss: 0.0197\n",
        "Epoch 867, Loss: 0.0249\n",
        "Epoch 868, Loss: 0.0288\n",
        "Epoch 869, Loss: 0.0259\n",
        "Epoch 870, Loss: 0.0244\n",
        "Epoch 871, Loss: 0.0229\n",
        "Epoch 872, Loss: 0.0273\n",
        "Epoch 873, Loss: 0.0254\n",
        "Epoch 874, Loss: 0.0222\n",
        "Epoch 875, Loss: 0.0246\n",
        "Epoch 876, Loss: 0.0269\n",
        "Epoch 877, Loss: 0.0237\n",
        "Epoch 878, Loss: 0.0275\n",
        "Epoch 879, Loss: 0.0267\n",
        "Epoch 880, Loss: 0.0241\n",
        "Epoch 881, Loss: 0.0220\n",
        "Epoch 882, Loss: 0.0263\n",
        "Epoch 883, Loss: 0.0226\n",
        "Epoch 884, Loss: 0.0242\n",
        "Epoch 885, Loss: 0.0242\n",
        "Epoch 886, Loss: 0.0235\n",
        "Epoch 887, Loss: 0.0252\n",
        "Epoch 888, Loss: 0.0232\n",
        "Epoch 889, Loss: 0.0238\n",
        "Epoch 890, Loss: 0.0226\n",
        "Epoch 891, Loss: 0.0236\n",
        "Epoch 892, Loss: 0.0258\n",
        "Epoch 893, Loss: 0.0266\n",
        "Epoch 894, Loss: 0.0243\n",
        "Epoch 895, Loss: 0.0273\n",
        "Epoch 896, Loss: 0.0243\n",
        "Epoch 897, Loss: 0.0223\n",
        "Epoch 898, Loss: 0.0213\n",
        "Epoch 899, Loss: 0.0234\n",
        "Epoch 900, Loss: 0.0234\n",
        "Epoch 901, Loss: 0.0260\n",
        "Epoch 902, Loss: 0.0252\n",
        "Epoch 903, Loss: 0.0236\n",
        "Epoch 904, Loss: 0.0229\n",
        "Epoch 905, Loss: 0.0223\n",
        "Epoch 906, Loss: 0.0250\n",
        "Epoch 907, Loss: 0.0236\n",
        "Epoch 908, Loss: 0.0273\n",
        "Epoch 909, Loss: 0.0259\n",
        "Epoch 910, Loss: 0.0189\n",
        "Epoch 911, Loss: 0.0207\n",
        "Epoch 912, Loss: 0.0264\n",
        "Epoch 913, Loss: 0.0196\n",
        "Epoch 914, Loss: 0.0261\n",
        "Epoch 915, Loss: 0.0268\n",
        "Epoch 916, Loss: 0.0228\n",
        "Epoch 917, Loss: 0.0267\n",
        "Epoch 918, Loss: 0.0266\n",
        "Epoch 919, Loss: 0.0195\n",
        "Epoch 920, Loss: 0.0210\n",
        "Epoch 921, Loss: 0.0236\n",
        "Epoch 922, Loss: 0.0229\n",
        "Epoch 923, Loss: 0.0231\n",
        "Epoch 924, Loss: 0.0237\n",
        "Epoch 925, Loss: 0.0244\n",
        "Epoch 926, Loss: 0.0229\n",
        "Epoch 927, Loss: 0.0234\n",
        "Epoch 928, Loss: 0.0207\n",
        "Epoch 929, Loss: 0.0238\n",
        "Epoch 930, Loss: 0.0224\n",
        "Epoch 931, Loss: 0.0238\n",
        "Epoch 932, Loss: 0.0250\n",
        "Epoch 933, Loss: 0.0182\n",
        "Epoch 934, Loss: 0.0224\n",
        "Epoch 935, Loss: 0.0219\n",
        "Epoch 936, Loss: 0.0232\n",
        "Epoch 937, Loss: 0.0232\n",
        "Epoch 938, Loss: 0.0218\n",
        "Epoch 939, Loss: 0.0228\n",
        "Epoch 940, Loss: 0.0213\n",
        "Epoch 941, Loss: 0.0222\n",
        "Epoch 942, Loss: 0.0261\n",
        "Epoch 943, Loss: 0.0208\n",
        "Epoch 944, Loss: 0.0265\n",
        "Epoch 945, Loss: 0.0257\n",
        "Epoch 946, Loss: 0.0226\n",
        "Epoch 947, Loss: 0.0247\n",
        "Epoch 948, Loss: 0.0224\n",
        "Epoch 949, Loss: 0.0199\n",
        "Epoch 950, Loss: 0.0270\n",
        "Epoch 951, Loss: 0.0271\n",
        "Epoch 952, Loss: 0.0225\n",
        "Epoch 953, Loss: 0.0205\n",
        "Epoch 954, Loss: 0.0253\n",
        "Epoch 955, Loss: 0.0213\n",
        "Epoch 956, Loss: 0.0256\n",
        "Epoch 957, Loss: 0.0210\n",
        "Epoch 958, Loss: 0.0236\n",
        "Epoch 959, Loss: 0.0238\n",
        "Epoch 960, Loss: 0.0244\n",
        "Epoch 961, Loss: 0.0225\n",
        "Epoch 962, Loss: 0.0251\n",
        "Epoch 963, Loss: 0.0209\n",
        "Epoch 964, Loss: 0.0210\n",
        "Epoch 965, Loss: 0.0264\n",
        "Epoch 966, Loss: 0.0242\n",
        "Epoch 967, Loss: 0.0250\n",
        "Epoch 968, Loss: 0.0223\n",
        "Epoch 969, Loss: 0.0236\n",
        "Epoch 970, Loss: 0.0233\n",
        "Epoch 971, Loss: 0.0224\n",
        "Epoch 972, Loss: 0.0261\n",
        "Epoch 973, Loss: 0.0212\n",
        "Epoch 974, Loss: 0.0244\n",
        "Epoch 975, Loss: 0.0215\n",
        "Epoch 976, Loss: 0.0199\n",
        "Epoch 977, Loss: 0.0221\n",
        "Epoch 978, Loss: 0.0211\n",
        "Epoch 979, Loss: 0.0219\n",
        "Epoch 980, Loss: 0.0202\n",
        "Epoch 981, Loss: 0.0239\n",
        "Epoch 982, Loss: 0.0206\n",
        "Epoch 983, Loss: 0.0232\n",
        "Epoch 984, Loss: 0.0223\n",
        "Epoch 985, Loss: 0.0248\n",
        "Epoch 986, Loss: 0.0283\n",
        "Epoch 987, Loss: 0.0227\n",
        "Epoch 988, Loss: 0.0248\n",
        "Epoch 989, Loss: 0.0223\n",
        "Epoch 990, Loss: 0.0250\n",
        "Epoch 991, Loss: 0.0246\n",
        "Epoch 992, Loss: 0.0231\n",
        "Epoch 993, Loss: 0.0281\n",
        "Epoch 994, Loss: 0.0210\n",
        "Epoch 995, Loss: 0.0218\n",
        "Epoch 996, Loss: 0.0220\n",
        "Epoch 997, Loss: 0.0229\n",
        "Epoch 998, Loss: 0.0240\n",
        "Epoch 999, Loss: 0.0209\n",
        "Epoch 1000, Loss: 0.0236\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4ngUnOLyDnh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values_exp2 = \"\"\"Epoch 1/1000, Loss: 0.9613, Learning Rate: 0.001000\n",
        "Epoch 2/1000, Loss: 0.9607, Learning Rate: 0.001000\n",
        "Epoch 3/1000, Loss: 0.9600, Learning Rate: 0.001000\n",
        "Epoch 4/1000, Loss: 0.9594, Learning Rate: 0.001000\n",
        "Epoch 5/1000, Loss: 0.9587, Learning Rate: 0.001000\n",
        "Epoch 6/1000, Loss: 0.9579, Learning Rate: 0.001000\n",
        "Epoch 7/1000, Loss: 0.9570, Learning Rate: 0.001000\n",
        "Epoch 8/1000, Loss: 0.9560, Learning Rate: 0.001000\n",
        "Epoch 9/1000, Loss: 0.9549, Learning Rate: 0.001000\n",
        "Epoch 10/1000, Loss: 0.9537, Learning Rate: 0.001000\n",
        "Epoch 11/1000, Loss: 0.9523, Learning Rate: 0.001000\n",
        "Epoch 12/1000, Loss: 0.9507, Learning Rate: 0.001000\n",
        "Epoch 13/1000, Loss: 0.9489, Learning Rate: 0.001000\n",
        "Epoch 14/1000, Loss: 0.9469, Learning Rate: 0.001000\n",
        "Epoch 15/1000, Loss: 0.9445, Learning Rate: 0.001000\n",
        "Epoch 16/1000, Loss: 0.9419, Learning Rate: 0.001000\n",
        "Epoch 17/1000, Loss: 0.9389, Learning Rate: 0.001000\n",
        "Epoch 18/1000, Loss: 0.9356, Learning Rate: 0.001000\n",
        "Epoch 19/1000, Loss: 0.9319, Learning Rate: 0.001000\n",
        "Epoch 20/1000, Loss: 0.9278, Learning Rate: 0.001000\n",
        "Epoch 21/1000, Loss: 0.9232, Learning Rate: 0.001000\n",
        "Epoch 22/1000, Loss: 0.9180, Learning Rate: 0.001000\n",
        "Epoch 23/1000, Loss: 0.9125, Learning Rate: 0.001000\n",
        "Epoch 24/1000, Loss: 0.9063, Learning Rate: 0.001000\n",
        "Epoch 25/1000, Loss: 0.8997, Learning Rate: 0.001000\n",
        "Epoch 26/1000, Loss: 0.8925, Learning Rate: 0.001000\n",
        "Epoch 27/1000, Loss: 0.8847, Learning Rate: 0.001000\n",
        "Epoch 28/1000, Loss: 0.8765, Learning Rate: 0.001000\n",
        "Epoch 29/1000, Loss: 0.8677, Learning Rate: 0.001000\n",
        "Epoch 30/1000, Loss: 0.8583, Learning Rate: 0.001000\n",
        "Epoch 31/1000, Loss: 0.8487, Learning Rate: 0.001000\n",
        "Epoch 32/1000, Loss: 0.8384, Learning Rate: 0.001000\n",
        "Epoch 33/1000, Loss: 0.8281, Learning Rate: 0.001000\n",
        "Epoch 34/1000, Loss: 0.8173, Learning Rate: 0.001000\n",
        "Epoch 35/1000, Loss: 0.8066, Learning Rate: 0.001000\n",
        "Epoch 36/1000, Loss: 0.7961, Learning Rate: 0.001000\n",
        "Epoch 37/1000, Loss: 0.7855, Learning Rate: 0.001000\n",
        "Epoch 38/1000, Loss: 0.7751, Learning Rate: 0.001000\n",
        "Epoch 39/1000, Loss: 0.7654, Learning Rate: 0.001000\n",
        "Epoch 40/1000, Loss: 0.7559, Learning Rate: 0.001000\n",
        "Epoch 41/1000, Loss: 0.7475, Learning Rate: 0.001000\n",
        "Epoch 42/1000, Loss: 0.7392, Learning Rate: 0.001000\n",
        "Epoch 43/1000, Loss: 0.7322, Learning Rate: 0.001000\n",
        "Epoch 44/1000, Loss: 0.7255, Learning Rate: 0.001000\n",
        "Epoch 45/1000, Loss: 0.7199, Learning Rate: 0.001000\n",
        "Epoch 46/1000, Loss: 0.7146, Learning Rate: 0.001000\n",
        "Epoch 47/1000, Loss: 0.7102, Learning Rate: 0.001000\n",
        "Epoch 48/1000, Loss: 0.7064, Learning Rate: 0.001000\n",
        "Epoch 49/1000, Loss: 0.7032, Learning Rate: 0.001000\n",
        "Epoch 50/1000, Loss: 0.7005, Learning Rate: 0.001000\n",
        "Epoch 51/1000, Loss: 0.6981, Learning Rate: 0.001000\n",
        "Epoch 52/1000, Loss: 0.6963, Learning Rate: 0.001000\n",
        "Epoch 53/1000, Loss: 0.6946, Learning Rate: 0.001000\n",
        "Epoch 54/1000, Loss: 0.6932, Learning Rate: 0.001000\n",
        "Epoch 55/1000, Loss: 0.6921, Learning Rate: 0.001000\n",
        "Epoch 56/1000, Loss: 0.6912, Learning Rate: 0.001000\n",
        "Epoch 57/1000, Loss: 0.6904, Learning Rate: 0.001000\n",
        "Epoch 58/1000, Loss: 0.6898, Learning Rate: 0.001000\n",
        "Epoch 59/1000, Loss: 0.6892, Learning Rate: 0.001000\n",
        "Epoch 60/1000, Loss: 0.6888, Learning Rate: 0.001000\n",
        "Epoch 61/1000, Loss: 0.6884, Learning Rate: 0.001000\n",
        "Epoch 62/1000, Loss: 0.6880, Learning Rate: 0.001000\n",
        "Epoch 63/1000, Loss: 0.6878, Learning Rate: 0.001000\n",
        "Epoch 64/1000, Loss: 0.6876, Learning Rate: 0.001000\n",
        "Epoch 65/1000, Loss: 0.6873, Learning Rate: 0.001000\n",
        "Epoch 66/1000, Loss: 0.6872, Learning Rate: 0.001000\n",
        "Epoch 67/1000, Loss: 0.6870, Learning Rate: 0.001000\n",
        "Epoch 68/1000, Loss: 0.6869, Learning Rate: 0.001000\n",
        "Epoch 69/1000, Loss: 0.6868, Learning Rate: 0.001000\n",
        "Epoch 70/1000, Loss: 0.6867, Learning Rate: 0.001000\n",
        "Epoch 71/1000, Loss: 0.6866, Learning Rate: 0.001000\n",
        "Epoch 72/1000, Loss: 0.6865, Learning Rate: 0.001000\n",
        "Epoch 73/1000, Loss: 0.6864, Learning Rate: 0.001000\n",
        "Epoch 74/1000, Loss: 0.6864, Learning Rate: 0.001000\n",
        "Epoch 75/1000, Loss: 0.6863, Learning Rate: 0.001000\n",
        "Epoch 76/1000, Loss: 0.6863, Learning Rate: 0.001000\n",
        "Epoch 77/1000, Loss: 0.6862, Learning Rate: 0.001000\n",
        "Epoch 78/1000, Loss: 0.6862, Learning Rate: 0.001000\n",
        "Epoch 79/1000, Loss: 0.6862, Learning Rate: 0.001000\n",
        "Epoch 80/1000, Loss: 0.6861, Learning Rate: 0.001000\n",
        "Epoch 81/1000, Loss: 0.6861, Learning Rate: 0.001000\n",
        "Epoch 82/1000, Loss: 0.6861, Learning Rate: 0.001000\n",
        "Epoch 83/1000, Loss: 0.6861, Learning Rate: 0.001000\n",
        "Epoch 84/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 85/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 86/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 87/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 88/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 89/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 90/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 91/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 92/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 93/1000, Loss: 0.6860, Learning Rate: 0.001000\n",
        "Epoch 94/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 95/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 96/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 97/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 98/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 99/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 100/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 101/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 102/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 103/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 104/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 105/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 106/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 107/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 108/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 109/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 110/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 111/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 112/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 113/1000, Loss: 0.6859, Learning Rate: 0.001000\n",
        "Epoch 114/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 115/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 116/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 117/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 118/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 119/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 120/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 121/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 122/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 123/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 124/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 125/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 126/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 127/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 128/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 129/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 130/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 131/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 132/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 133/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 134/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 135/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 136/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 137/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 138/1000, Loss: 0.6858, Learning Rate: 0.001000\n",
        "Epoch 139/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 140/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 141/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 142/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 143/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 144/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 145/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 146/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 147/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 148/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 149/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 150/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 151/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 152/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 153/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 154/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 155/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 156/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 157/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 158/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 159/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 160/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 161/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 162/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 163/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 164/1000, Loss: 0.6857, Learning Rate: 0.001000\n",
        "Epoch 165/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 166/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 167/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 168/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 169/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 170/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 171/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 172/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 173/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 174/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 175/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 176/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 177/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 178/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 179/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 180/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 181/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 182/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 183/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 184/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 185/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 186/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 187/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 188/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 189/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 190/1000, Loss: 0.6856, Learning Rate: 0.001000\n",
        "Epoch 191/1000, Loss: 0.6856, Learning Rate: 0.000500\n",
        "Epoch 192/1000, Loss: 0.6856, Learning Rate: 0.000500\n",
        "Epoch 193/1000, Loss: 0.6856, Learning Rate: 0.000500\n",
        "Epoch 194/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 195/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 196/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 197/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 198/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 199/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 200/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 201/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 202/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 203/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 204/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 205/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 206/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 207/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 208/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 209/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 210/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 211/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 212/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 213/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 214/1000, Loss: 0.6855, Learning Rate: 0.000500\n",
        "Epoch 215/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 216/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 217/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 218/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 219/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 220/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 221/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 222/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 223/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 224/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 225/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 226/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 227/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 228/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 229/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 230/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 231/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 232/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 233/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 234/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 235/1000, Loss: 0.6855, Learning Rate: 0.000250\n",
        "Epoch 236/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 237/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 238/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 239/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 240/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 241/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 242/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 243/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 244/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 245/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 246/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 247/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 248/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 249/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 250/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 251/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 252/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 253/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 254/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 255/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 256/1000, Loss: 0.6855, Learning Rate: 0.000125\n",
        "Epoch 257/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 258/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 259/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 260/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 261/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 262/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 263/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 264/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 265/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 266/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 267/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 268/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 269/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 270/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 271/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 272/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 273/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 274/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 275/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 276/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 277/1000, Loss: 0.6855, Learning Rate: 0.000063\n",
        "Epoch 278/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 279/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 280/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 281/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 282/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 283/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 284/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 285/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 286/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 287/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 288/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 289/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 290/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 291/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 292/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 293/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 294/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 295/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 296/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 297/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 298/1000, Loss: 0.6855, Learning Rate: 0.000031\n",
        "Epoch 299/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 300/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 301/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 302/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 303/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 304/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 305/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 306/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 307/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 308/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 309/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 310/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 311/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 312/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 313/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 314/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 315/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 316/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 317/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 318/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 319/1000, Loss: 0.6855, Learning Rate: 0.000016\n",
        "Epoch 320/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 321/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 322/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 323/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 324/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 325/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 326/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 327/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 328/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 329/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 330/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 331/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 332/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 333/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 334/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 335/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 336/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 337/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 338/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 339/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 340/1000, Loss: 0.6855, Learning Rate: 0.000008\n",
        "Epoch 341/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 342/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 343/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 344/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 345/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 346/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 347/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 348/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 349/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 350/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 351/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 352/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 353/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 354/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 355/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 356/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 357/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 358/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 359/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 360/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 361/1000, Loss: 0.6855, Learning Rate: 0.000004\n",
        "Epoch 362/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 363/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 364/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 365/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 366/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 367/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 368/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 369/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 370/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 371/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 372/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 373/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 374/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 375/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 376/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 377/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 378/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 379/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 380/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 381/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 382/1000, Loss: 0.6855, Learning Rate: 0.000002\n",
        "Epoch 383/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 384/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 385/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 386/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 387/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 388/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 389/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 390/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 391/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 392/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 393/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 394/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 395/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 396/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 397/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 398/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 399/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 400/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 401/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 402/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 403/1000, Loss: 0.6855, Learning Rate: 0.000001\n",
        "Epoch 404/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 405/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 406/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 407/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 408/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 409/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 410/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 411/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 412/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 413/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 414/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 415/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 416/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 417/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 418/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 419/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 420/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 421/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 422/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 423/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 424/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 425/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 426/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 427/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 428/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 429/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 430/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 431/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 432/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 433/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 434/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 435/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 436/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 437/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 438/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 439/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 440/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 441/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 442/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 443/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 444/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 445/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 446/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 447/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 448/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 449/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 450/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 451/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 452/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 453/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 454/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 455/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 456/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 457/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 458/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 459/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 460/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 461/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 462/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 463/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 464/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 465/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 466/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 467/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 468/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 469/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 470/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 471/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 472/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 473/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 474/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 475/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 476/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 477/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 478/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 479/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 480/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 481/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 482/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 483/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 484/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 485/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 486/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 487/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 488/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 489/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 490/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 491/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 492/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 493/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 494/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 495/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 496/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 497/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 498/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 499/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 500/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 501/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 502/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 503/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 504/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 505/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 506/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 507/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 508/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 509/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 510/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 511/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 512/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 513/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 514/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 515/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 516/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 517/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 518/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 519/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 520/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 521/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 522/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 523/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 524/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 525/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 526/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 527/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 528/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 529/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 530/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 531/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 532/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 533/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 534/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 535/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 536/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 537/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 538/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 539/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 540/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 541/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 542/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 543/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 544/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 545/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 546/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 547/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 548/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 549/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 550/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 551/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 552/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 553/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 554/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 555/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 556/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 557/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 558/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 559/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 560/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 561/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 562/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 563/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 564/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 565/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 566/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 567/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 568/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 569/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 570/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 571/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 572/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 573/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 574/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 575/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 576/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 577/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 578/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 579/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 580/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 581/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 582/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 583/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 584/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 585/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 586/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 587/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 588/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 589/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 590/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 591/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 592/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 593/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 594/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 595/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 596/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 597/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 598/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 599/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 600/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 601/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 602/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 603/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 604/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 605/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 606/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 607/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 608/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 609/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 610/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 611/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 612/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 613/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 614/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 615/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 616/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 617/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 618/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 619/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 620/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 621/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 622/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 623/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 624/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 625/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 626/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 627/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 628/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 629/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 630/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 631/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 632/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 633/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 634/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 635/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 636/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 637/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 638/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 639/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 640/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 641/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 642/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 643/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 644/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 645/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 646/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 647/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 648/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 649/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 650/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 651/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 652/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 653/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 654/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 655/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 656/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 657/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 658/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 659/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 660/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 661/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 662/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 663/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 664/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 665/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 666/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 667/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 668/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 669/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 670/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 671/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 672/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 673/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 674/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 675/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 676/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 677/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 678/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 679/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 680/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 681/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 682/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 683/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 684/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 685/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 686/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 687/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 688/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 689/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 690/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 691/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 692/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 693/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 694/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 695/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 696/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 697/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 698/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 699/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 700/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 701/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 702/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 703/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 704/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 705/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 706/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 707/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 708/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 709/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 710/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 711/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 712/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 713/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 714/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 715/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 716/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 717/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 718/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 719/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 720/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 721/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 722/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 723/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 724/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 725/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 726/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 727/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 728/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 729/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 730/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 731/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 732/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 733/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 734/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 735/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 736/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 737/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 738/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 739/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 740/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 741/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 742/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 743/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 744/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 745/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 746/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 747/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 748/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 749/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 750/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 751/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 752/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 753/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 754/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 755/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 756/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 757/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 758/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 759/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 760/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 761/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 762/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 763/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 764/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 765/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 766/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 767/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 768/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 769/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 770/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 771/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 772/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 773/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 774/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 775/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 776/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 777/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 778/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 779/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 780/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 781/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 782/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 783/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 784/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 785/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 786/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 787/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 788/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 789/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 790/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 791/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 792/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 793/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 794/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 795/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 796/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 797/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 798/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 799/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 800/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 801/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 802/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 803/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 804/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 805/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 806/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 807/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 808/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 809/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 810/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 811/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 812/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 813/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 814/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 815/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 816/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 817/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 818/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 819/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 820/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 821/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 822/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 823/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 824/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 825/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 826/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 827/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 828/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 829/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 830/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 831/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 832/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 833/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 834/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 835/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 836/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 837/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 838/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 839/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 840/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 841/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 842/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 843/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 844/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 845/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 846/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 847/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 848/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 849/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 850/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 851/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 852/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 853/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 854/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 855/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 856/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 857/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 858/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 859/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 860/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 861/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 862/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 863/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 864/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 865/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 866/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 867/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 868/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 869/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 870/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 871/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 872/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 873/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 874/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 875/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 876/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 877/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 878/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 879/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 880/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 881/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 882/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 883/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 884/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 885/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 886/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 887/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 888/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 889/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 890/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 891/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 892/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 893/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 894/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 895/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 896/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 897/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 898/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 899/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 900/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 901/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 902/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 903/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 904/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 905/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 906/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 907/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 908/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 909/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 910/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 911/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 912/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 913/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 914/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 915/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 916/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 917/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 918/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 919/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 920/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 921/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 922/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 923/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 924/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 925/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 926/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 927/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 928/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 929/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 930/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 931/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 932/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 933/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 934/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 935/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 936/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 937/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 938/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 939/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 940/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 941/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 942/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 943/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 944/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 945/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 946/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 947/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 948/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 949/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 950/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 951/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 952/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 953/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 954/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 955/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 956/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 957/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 958/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 959/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 960/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 961/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 962/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 963/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 964/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 965/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 966/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 967/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 968/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 969/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 970/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 971/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 972/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 973/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 974/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 975/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 976/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 977/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 978/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 979/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 980/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 981/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 982/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 983/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 984/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 985/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 986/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 987/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 988/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 989/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 990/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 991/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 992/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 993/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 994/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 995/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 996/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 997/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 998/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 999/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "Epoch 1000/1000, Loss: 0.6855, Learning Rate: 0.000000\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3fvysYwfPJvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values = parse_loss_values(loss_values_exp1)\n",
        "print(loss_values)"
      ],
      "metadata": {
        "id": "yCQ50D7bOvTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values_2 = parse_loss_values(loss_values_exp2)\n",
        "print(loss_values_2)"
      ],
      "metadata": {
        "id": "LQGQhj36Pi2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(training_losses, validation_losses, save_path=None):\n",
        "    \"\"\"\n",
        "    Plots both training and validation loss over epochs and optionally saves the plot to a file.\n",
        "\n",
        "    Parameters:\n",
        "    - training_losses (list): A list of training loss values recorded at each epoch.\n",
        "    - validation_losses (list): A list of validation loss values recorded at each epoch.\n",
        "    - save_path (str, optional): The path to save the plot image. If None, the plot is not saved.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(training_losses) + 1)  # Generate the x-axis (epoch numbers)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot the training loss\n",
        "    plt.plot(epochs, training_losses, label='AG News', color='blue')\n",
        "\n",
        "    # Plot the validation loss\n",
        "    plt.plot(epochs, validation_losses, label='Reuters', color='red')\n",
        "\n",
        "    plt.title('Loss Function Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the figure if a path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, format='png')  # Save the plot as a PNG file\n",
        "        print(f\"Plot saved as {save_path}\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "# Save the plot as 'loss_plot.png'\n",
        "plot_loss(loss_values, loss_values_2, save_path='loss_plot.png')\n"
      ],
      "metadata": {
        "id": "lTLM6mfTOCDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "4cNkoLfhY13V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the AG News Subset (this is a placeholder, as AG News is not directly available in sklearn)\n",
        "# For AG News, you would typically load it from a file or a custom dataset loader\n",
        "# Here we use 20 Newsgroups as an example\n",
        "newsgroups = fetch_20newsgroups(subset='train')\n",
        "\n",
        "# Initialize a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "\n",
        "# Get the number of unique tokens\n",
        "num_unique_tokens = len(vectorizer.get_feature_names_out())\n",
        "print(f\"Number of unique tokens: {num_unique_tokens}\")"
      ],
      "metadata": {
        "id": "y47O1z8TZdLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_parquet(\"hf://datasets/celsowm/bbc_news_ptbr/data/train-00000-of-00001-97102f0adab65e78.parquet\")\n",
        "\n",
        "# Assuming the text is in a column named 'text' (replace with the actual column name if different)\n",
        "texts = df['texto'].tolist()\n",
        "\n",
        "# Initialize a CountVectorizer to tokenize and count unique tokens\n",
        "vectorizer = CountVectorizer(lowercase=True, token_pattern=r'\\b\\w+\\b')  # Adjust token_pattern as needed\n",
        "\n",
        "# Fit the vectorizer on the text data\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Get the number of unique tokens\n",
        "num_unique_tokens = len(vectorizer.get_feature_names_out())\n",
        "print(f\"Number of unique tokens: {num_unique_tokens}\")"
      ],
      "metadata": {
        "id": "novhw2LNaRuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "Qi9JRpBhaiGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the triplets (head, relation, tail)\n",
        "# Modified triplets to ensure they are connected\n",
        "triplets = [\n",
        "    (\"U.S stock future\", \"increased\", \"Tuesday\"),\n",
        "    (\"Tuesday\", \"marked\", \"opening\"),\n",
        "    (\"opening\", \"showed\", \"stable\"),\n",
        "    (\"stable\", \"related_to\", \"data\"),\n",
        "    (\"data\", \"showed\", \"growth\"),\n",
        "    (\"U.S stock future\", \"showed\", \"growth\")\n",
        "]\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges to the graph based on the triplets\n",
        "for head, relation, tail in triplets:\n",
        "    G.add_edge(head, tail, label=relation)\n",
        "\n",
        "# Draw the graph\n",
        "pos = nx.spring_layout(G)  # Layout for positioning nodes\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue')\n",
        "\n",
        "# Draw edges\n",
        "nx.draw_networkx_edges(G, pos, edge_color='gray', arrowstyle='->', arrowsize=20)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold')\n",
        "\n",
        "# Draw edge labels\n",
        "edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10)\n",
        "\n",
        "# Display the graph\n",
        "plt.title(\"Graph Visualization\")\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.savefig(\"knowledge_graph.png\", dpi=300, bbox_inches='tight', format='png')  # Save as PNG with high DPI\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n3pANMGGqRFm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}